{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T02:49:35.339793Z",
     "start_time": "2020-05-25T02:49:30.563526Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-25T02:39:37.262Z"
    },
    "code_folding": [
     2,
     62,
     102,
     125,
     130,
     132,
     139,
     143,
     147,
     234,
     253,
     272,
     324
    ]
   },
   "outputs": [],
   "source": [
    "# new\n",
    "class FFL():\n",
    "    \"\"\"\n",
    "        A class to create a single input/hidden feedforward layer of given shape.\n",
    "        \n",
    "        Input:\n",
    "        -------\n",
    "        n_input: Number of input values to layer. If none, tried to check from preious layer.\n",
    "        neurons: Neurons on that layer.\n",
    "        bias: External bias numpy array. If None, used from np.random.randn(neurons)\n",
    "        weights: External weight numpy array. If None, used from np.random.randn(n_input, neurons)\n",
    "        activation: One of [\"relu\", \"sigmoid\", \"tanh\", \"softmax\"].\n",
    "        is_bias: Do we want bias to be used here? Default True.\n",
    "        \n",
    "        Output:\n",
    "        --------\n",
    "        Object of FFL which can be stacked later to use.\n",
    "        \n",
    "        Example:\n",
    "        FFL(2, 2, activation=\"sigmoid\")\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape=None, neurons=1, bias=None, weights=None, activation=None, is_bias = True):\n",
    "        \n",
    "        np.random.seed(100)\n",
    "        self.input_shape = input_shape\n",
    "        self.neurons = neurons\n",
    "        self.isbias = is_bias\n",
    "        self.name = \"\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        if input_shape != None:\n",
    "            self.output_shape = neurons\n",
    "        \n",
    "        \n",
    "        if self.input_shape != None:\n",
    "            self.weights = weights if weights != None else np.random.randn(self.input_shape, neurons)\n",
    "            self.parameters = self.input_shape *  self.neurons + self.neurons if self.isbias else 0  \n",
    "        if(is_bias):\n",
    "            self.biases = bias if bias != None else np.random.randn(neurons)\n",
    "        else:\n",
    "            self.biases = 0\n",
    "            \n",
    "        self.out = None\n",
    "        self.input = None\n",
    "        self.error = None\n",
    "        self.delta = None\n",
    "        activations = [\"relu\", \"sigmoid\", \"tanh\", \"softmax\"]\n",
    "        self.delta_weights = 0\n",
    "        self.delta_biases = 0\n",
    "        self.pdelta_weights = 0\n",
    "        self.pdelta_biases = 0\n",
    "        \n",
    "        #self.bias = np.ones(self.bias.shape)\n",
    "        \n",
    "        if activation not in activations and activation != None:\n",
    "             raise ValueError(f\"Activation function not recognised. Use one of {activations} instead.\")\n",
    "        else:\n",
    "            self.activation = activation\n",
    "            \n",
    "           \n",
    "    def activation_dfn(self, r):\n",
    "        \"\"\"\n",
    "            A method of FFL to find derivative of given activation function.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        if self.activation is None:\n",
    "            return np.ones(r.shape)\n",
    "\n",
    "        if self.activation == 'tanh':\n",
    "            return 1 - r ** 3\n",
    "\n",
    "        if self.activation == 'sigmoid':\n",
    "            return r * (1 - r)\n",
    "\n",
    "        if self.activation == \"softmax\":\n",
    "            soft = self.activation_fn(r)\n",
    "                    \n",
    "            #s = soft.reshape(-1, 1)\n",
    "            #dsoft = np.diagflat(s) - np.dot(s, s.T)\n",
    "            #diag_soft = dsoft.diagonal()\n",
    "            \n",
    "            # take only the diagonal of dsoft i.e i==j only\n",
    "            \"\"\"\n",
    "                soft = a / np.sum(a)\n",
    "\n",
    "                dsoft = np.diag(soft)\n",
    "                for i in range(len(x)):\n",
    "                    for j in range(len(soft)):\n",
    "                        if i == j:\n",
    "                            d = 1\n",
    "                        else:\n",
    "                            d = 0\n",
    "                        dsoft[i][j] = soft[i] * (d - soft[j])\n",
    "            \"\"\"\n",
    "            diag_soft = soft*(1- soft)\n",
    "            return diag_soft\n",
    "        \n",
    "        return r\n",
    "\n",
    "    def activation_fn(self, r):\n",
    "        \"\"\"\n",
    "        A method of FFL which contains the operation and defination of given activation function.\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.activation == None or self.activation == \"linear\":\n",
    "            return r\n",
    "\n",
    "        # tanh\n",
    "        if self.activation == 'tanh':\n",
    "            return np.tanh(r)\n",
    "\n",
    "        # sigmoid\n",
    "        if self.activation == 'sigmoid':\n",
    "            \n",
    "            return 1 / (1 + np.exp(-r))\n",
    "\n",
    "        if self.activation == \"softmax\":\n",
    "            # stable softmax\n",
    "            r = r - np.max(r)\n",
    "            s = np.exp(r)\n",
    "            return s / np.sum(s)\n",
    "        \n",
    "    def apply_activation(self, x):\n",
    "        soma = np.dot(x, self.weights) + self.biases\n",
    "        self.out = self.activation_fn(soma)\n",
    "        \n",
    "        return self.out\n",
    "    def set_n_input(self):\n",
    "        self.weights = self.w if self.w != None else np.random.normal(size=(self.n_input, self.neurons))\n",
    "    def backpropagate(self, nx_layer):\n",
    "        self.error = np.dot(nx_layer.weights, nx_layer.delta)\n",
    "        self.delta = self.error * self.activation_dfn(self.out)\n",
    "        self.delta_weights += self.delta * np.atleast_2d(self.input).T\n",
    "        self.delta_biases += self.delta\n",
    "    \n",
    "    # added below methods from cnn\n",
    "    def set_output_shape(self):\n",
    "        self.set_n_input()\n",
    "        self.output_shape = self.neurons\n",
    "        self.get_parameters()\n",
    "    def get_parameters(self):\n",
    "        self.parameters = self.input_shape *  self.neurons + self.neurons if self.isbias else 0  \n",
    "        return self.parameters\n",
    "# new NN\n",
    "class NN:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.info_df = {}\n",
    "        self.column = [\"LName\", \"Input\", \"Output\", \"Activation\", \"Bias\"]\n",
    "        self.parameters = 0\n",
    "        self.optimizer = \"\"\n",
    "        self.loss = \"\"\n",
    "        self.all_loss = {}\n",
    "        self.lr = 1\n",
    "        self.metrics = []\n",
    "        self.av_optimizers = [\"sgd\", \"iterative\", \"momentum\", \"rmsprop\", \"adagrad\", \"adam\", \"adamax\", \"adadelta\"]\n",
    "        self.av_metrics = [\"mse\", \"accuracy\", \"cse\"]\n",
    "        self.av_loss = [\"mse\", \"cse\"]\n",
    "        self.iscompiled = False\n",
    "    \n",
    "        self.batch_size = 8\n",
    "        self.mr = 0.0001\n",
    "        self.all_acc = []\n",
    "        self.eps = 1e-8\n",
    "    \n",
    "    def add(self, layer):\n",
    "        if(len(self.layers) > 0):\n",
    "            prev_layer = self.layers[-1]\n",
    "            if prev_layer.name != \"Input Layer\":\n",
    "                prev_layer.name = f\"Hidden Layer{len(self.layers) - 1}\" \n",
    "            if layer.input_shape == None:\n",
    "                layer.input_shape = prev_layer.output_shape\n",
    "                layer.set_output_shape()\n",
    "            layer.name = \"Output Layer\"\n",
    "            if prev_layer.neurons != layer.input_shape and layer.input_shape != None:\n",
    "                raise ValueError(f\"This layer '{layer.name}' must have neurons={prev_layer.neurons} because '{prev_layer.name}' has output of {prev_layer.neurons}.\")\n",
    "        else:\n",
    "            layer.name = \"Input Layer\"\n",
    "        #layer.parameters = layer.n_input * layer.neurons + layer.neurons * 1 if layer.isbias else 0\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    def summary(self):\n",
    "        lname = []\n",
    "        linput = []\n",
    "        lneurons = []\n",
    "        lactivation = []\n",
    "        lisbias = []\n",
    "        for layer in self.layers:\n",
    "            #self.layer_info.append(f\"\\t|| {layer.name} \\t || {layer.n_input} \\t || {layer.neurons} \\t|| {layer.activation} \\t || {layer.isbias} ||\\n\") \n",
    "            lname.append(layer.name)\n",
    "            linput.append(layer.input_shape)\n",
    "            lneurons.append(layer.neurons)\n",
    "            lactivation.append(layer.activation)\n",
    "            lisbias.append(layer.isbias)\n",
    "            \n",
    "            self.parameters += layer.parameters\n",
    "        model_dict = {\"Layer Name\": lname, \"Input\": linput, \"Neurons\": lneurons, \"Activation\": lactivation, \"Bias\": lisbias}    \n",
    "        model_df = pd.DataFrame(model_dict).set_index(\"Layer Name\")\n",
    "        #summ = \"\".join(self.layer_info)\n",
    "        #print(summ)\n",
    "        print(model_df)\n",
    "        print(\"Total Parameters: \", self.parameters)\n",
    "    \n",
    "    def visualize(self):\n",
    "        k = list(self.all_loss.keys())\n",
    "        v = list(self.all_loss.values())\n",
    "        plt.plot(k, v)\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(self.loss)\n",
    "        plt.show()\n",
    "    \n",
    "    def compile_model(self, lr=0.01, mr = 0.001, opt = \"sgd\", loss = \"mse\", metrics=['mse']):\n",
    "        if opt not in self.av_optimizers:\n",
    "            raise ValueError(f\"Optimizer is not understood, use one of {self.av_optimizers}.\")\n",
    "        \n",
    "        for m in metrics:\n",
    "            if m not in self.av_metrics:\n",
    "                raise ValueError(f\"Metrics is not understood, use one of {self.av_metrics}.\")\n",
    "        \n",
    "        if loss not in self.av_loss:\n",
    "            raise ValueError(f\"Loss function is not understood, use one of {self.av_loss}.\")\n",
    "        \n",
    "        #self.optimizer = opt\n",
    "        self.loss = loss\n",
    "        self.lr = lr\n",
    "        self.mr = mr\n",
    "        self.metrics = metrics\n",
    "        self.iscompiled = True\n",
    "        self.optimizer = Optimizer(layers=self.layers, name=opt, learning_rate=lr, mr=mr)\n",
    "        self.optimizer = self.optimizer.opt_dict[opt]\n",
    "        #print(self.optimizer)\n",
    "        \"\"\"\n",
    "            if opt == \"sgd\":\n",
    "                self.optimizer = Optimizer(layers=self.layers, name=\"sgd\", learning_rate=lr)\n",
    "                self.optimizer = self.optimizer.sgd\n",
    "            elif opt == \"adam\":\n",
    "                self.optimizer = Optimizer(layers=self.layers, name=\"adam\", learning_rate=lr)\n",
    "                self.optimizer = self.optimizer.adam\n",
    "                self.optimizer = self.ADAM\n",
    "                alpha = self.lr\n",
    "                beta1 = 0.9\n",
    "                beta2 = 0.999\n",
    "                epsilon = 1e-8\n",
    "                for l in self.layers:\n",
    "                    l.adam_param = []\n",
    "                    # w, a, b1, b2, eps, m, v, t\n",
    "                    l.adam_param = [[l.weights, alpha, beta1, beta2, epsilon, 0, 0, 0]]\n",
    "                    if l.isbias:\n",
    "                        l.adam_param.append([l.bias, alpha, beta1, beta2, epsilon, 0, 0, 0])\n",
    "        \"\"\"\n",
    "    def SGD(self, update):\n",
    "        for i in range(len(self.layers)):\n",
    "            layer = self.layers[i]       \n",
    "            #print(layer.delta.shape, layer.delta_weights.shape, layer.pdelta_biases)\n",
    "            input_to_use = np.atleast_2d(layer.input)\n",
    "            #dw = layer.delta * input_to_use[-1].T \n",
    "            #db =  layer.delta \n",
    "            if update:\n",
    "                layer.delta_weights /= self.batch_size\n",
    "                layer.delta_biases /= self.batch_size\n",
    "                #print(layer.delta_weights.shape, layer.weights.shape)\n",
    "                layer.weights += (layer.pdelta_weights * self.mr  + layer.delta_weights * self.lr)\n",
    "                layer.biases += (layer.pdelta_biases * self.mr  +  layer.delta_biases * self.lr)\n",
    "                layer.pdelta_weights = layer.delta_weights\n",
    "                layer.pdelta_biases = layer.delta_biases\n",
    "            #else:\n",
    "                layer.delta_weights = 0\n",
    "                layer.delta_biases = 0 \n",
    "   \n",
    "    def ADAM(self, update):\n",
    "        \n",
    "        for i in range(len(self.layers)):\n",
    "            layer = self.layers[i]         \n",
    "            input_to_use = np.atleast_2d(layer.input)\n",
    "            dw = layer.delta * input_to_use.T \n",
    "            db =  layer.delta \n",
    "            dparam = [dw, db]\n",
    "            nparam = []\n",
    "            l = layer\n",
    "            for i, grad in enumerate(dparam):\n",
    "                #print(i)\n",
    "                theta = l.adam_param[i][0]\n",
    "                alpha = l.adam_param[i][1]\n",
    "                beta1 = l.adam_param[i][2]\n",
    "                beta2 = l.adam_param[i][3]\n",
    "                epsilon = l.adam_param[i][4]\n",
    "                m = l.adam_param[i][5]\n",
    "                v = l.adam_param[i][6]\n",
    "                t = l.adam_param[i][7] + 1\n",
    "                             \n",
    "                m = beta1 * m + (1-beta1) * grad # moving avg of grad\n",
    "                v = beta2 * v + (1-beta2) * np.power(grad, 2) #\n",
    "                m_cap = m / (1 - np.power(beta1, t))\n",
    "                v_cap = v / (1 - np.power(beta2, t))\n",
    "                theta += (self.mr * layer.dprev[i]) + (alpha * np.nan_to_num(m_cap / (np.sqrt(v_cap) + epsilon)))\n",
    "                layer.dprev[i] = np.nan_to_num((m_cap / (np.sqrt(v_cap) + epsilon))) * 0\n",
    "                l.adam_param[i] = [theta, alpha, beta1, beta2, epsilon, m, v, t]\n",
    "                nparam.append(theta)\n",
    "            if len(nparam) == 2:\n",
    "                layer.weights = nparam[0]\n",
    "                layer.bias = nparam[1]\n",
    "            else:\n",
    "                layer.weights = nparam[0]\n",
    "            \n",
    "    def feedforward(self, x):\n",
    "        for l in self.layers:\n",
    "            l.input = x\n",
    "            \n",
    "            x = l.apply_activation(x)  \n",
    "            #print(l.name, \"ip\", l.input.shape, \"op\", x.shape)\n",
    "            #print(l.weights.shape, l.bias)\n",
    "            l.out = x\n",
    "            \n",
    "        return x\n",
    "        \n",
    "    def predict(self, X):\n",
    "        out = []\n",
    "        for x in X:\n",
    "            out.append(self.feedforward(x))\n",
    "        return out\n",
    "#         pass\n",
    "    def apply_loss(self, y, out):\n",
    "        if self.loss == \"mse\":\n",
    "            loss = y - out\n",
    "            mse = np.mean(np.square(loss))\n",
    "#             self.all_loss.append(mse)          \n",
    "            return loss, mse\n",
    "        if self.loss == 'cse':\n",
    "            \"\"\" Requires out to be probability values. \"\"\"     \n",
    "            if len(out) == len(y) == 1:\n",
    "                #print(\"Using Binary CSE.\")\n",
    "                #y += self.eps\n",
    "                #out += self.eps\n",
    "                cse = -(y * np.log(out) + (1 - y) * np.log(1 - out))\n",
    "                loss = -(y / out - (1 - y) / (1 - out))\n",
    "                #cse = np.mean(abs(cse))\n",
    "            else:\n",
    "                #print(\"Using Categorical CSE.\")\n",
    "                if self.layers[-1].activation == \"softmax\":\n",
    "                    # if o/p layer's fxn is softmax then loss is y - out\n",
    "                    # check the derivation of softmax and crossentropy with derivative\n",
    "                    loss = y - out\n",
    "                    loss = np.nan_to_num(loss / self.layers[-1].activation_dfn(out))\n",
    "                else:\n",
    "                    y = np.float64(y)\n",
    "                    #y += self.eps\n",
    "                    out += self.eps\n",
    "                    #cse =  -np.sum(y * (np.log(out)))\n",
    "                    \n",
    "                    loss = -(np.nan_to_num(y / out) - np.nan_to_num((1 - y) / (1 - out)))\n",
    "                \n",
    "            \n",
    "                cse = -np.sum((y * np.nan_to_num(np.log(out)) + (1 - y) * np.nan_to_num(np.log(1 - out))))\n",
    "            return loss, cse\n",
    "        \n",
    "    def backpropagate(self, loss, update = True):\n",
    "        \n",
    "        \n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            layer = self.layers[i]\n",
    "            if layer == self.layers[-1]:\n",
    "                layer.error = loss\n",
    "                \n",
    "                layer.delta = layer.error * layer.activation_dfn(layer.out) \n",
    "                #layer.pdelta_weights = layer.delta_weights\n",
    "                #layer.pdelta_biases = layer.delta_biases\n",
    "\n",
    "                layer.delta_weights += layer.delta * np.atleast_2d(layer.input).T\n",
    "                layer.delta_biases += layer.delta\n",
    "\n",
    "            else:\n",
    "                nx_layer = self.layers[i+1]\n",
    "                layer.backpropagate(nx_layer)\n",
    "            if update:\n",
    "                layer.delta_weights /= self.batch_size\n",
    "                layer.delta_biases /= self.batch_size\n",
    "        if update:\n",
    "            #self.SGD(update)       \n",
    "            self.optimizer(self.layers)\n",
    "            self.zerograd()\n",
    "    def zerograd(self):\n",
    "        for l in self.layers:\n",
    "            l.delta_weights=0\n",
    "            l.delta_biases = 0\n",
    "    def check_trainnable(self, X, Y):\n",
    "        if self.iscompiled == False:\n",
    "            raise ValueError(\"Model is not compiled.\")\n",
    "        if len(X) != len(Y):\n",
    "            raise ValueError(\"Length of training input and label is not equal.\")\n",
    "        if X[0].shape[0] != self.layers[0].input_shape:\n",
    "            layer = self.layers[0]\n",
    "            raise ValueError(f\"'{layer.name}' expects input of {layer.input_shape} while {X[0].shape[0]} is given.\")\n",
    "        if Y.shape[-1] != self.layers[-1].neurons:\n",
    "            op_layer = self.layers[-1]\n",
    "            raise ValueError(f\"'{op_layer.name}' expects input of {op_layer.neurons} while {Y.shape[-1]} is given.\")  \n",
    "        \n",
    "    def train(self, X, Y, epochs, show_every=1, batch_size = 32, shuffle=True):\n",
    "        self.check_trainnable(X, Y)\n",
    "        self.batch_size = batch_size\n",
    "        t1 = time.time()\n",
    "        self.losses = []\n",
    "        \n",
    "        len_batch = int(len(X)/batch_size)\n",
    "        \n",
    "        \n",
    "        batches = []\n",
    "        \n",
    "        curr_ind = np.arange(0, len(X), dtype=np.int32)\n",
    "        if shuffle: \n",
    "            np.random.shuffle(curr_ind)\n",
    "\n",
    "        if(len(curr_ind) % batch_size) != 0 :\n",
    "            nx = len(curr_ind) % batch_size\n",
    "            nx = curr_ind[:nx]\n",
    "            curr_ind = np.hstack([curr_ind, nx])\n",
    "        #print(curr_ind.shape, len_batch)\n",
    "        batches = np.split(curr_ind, batch_size)\n",
    "\n",
    "        #print(len(batches))\n",
    "        for e in range(epochs):            \n",
    "            \n",
    "            err = []\n",
    "            for batch in batches:\n",
    "                a = [] \n",
    "                curr_x, curr_y = X[batch], Y[batch]\n",
    "                b = 0\n",
    "                batch_loss = 0\n",
    "                for x, y in zip(curr_x, curr_y):\n",
    "                    out = self.feedforward(x)\n",
    "                    loss, error = self.apply_loss(y, out)\n",
    "                    #loss = loss.mean(axis=0)\n",
    "                    batch_loss += loss\n",
    "                    err.append(error)\n",
    "                    update = False\n",
    "                    if b == batch_size-1:\n",
    "                        update = True\n",
    "                        loss = batch_loss/batch_size\n",
    "                    self.backpropagate(loss, update)\n",
    "                    b+=1\n",
    "                \n",
    "\n",
    "            if e % show_every == 0:      \n",
    "                out = self.feedforward(X)\n",
    "                loss, error = self.apply_loss(Y, out)\n",
    "                #print(error)\n",
    "                out_activation = self.layers[-1].activation\n",
    "                print(out_activation)\n",
    "                if out_activation == \"softmax\":\n",
    "                    #print(out, Y)\n",
    "                    pred = out.argmax(axis=1) == Y.argmax(axis=1)\n",
    "                    #loss, error = self.apply_loss(Y, out)\n",
    "                elif out_activation == \"sigmoid\":\n",
    "                    pred = out > 0.7\n",
    "                    #pred = pred == Y\n",
    "                elif out_activation == None:\n",
    "                    pred = abs(Y-out) < 0.000001\n",
    "                    \n",
    "                self.all_loss[e] = loss\n",
    "                a = np.array(a)\n",
    "                err = np.array(err)\n",
    "                print(f\"Time: {round(time.time() - t1, 3)}sec\")\n",
    "                t1 = time.time()\n",
    "                print('Epoch: #%s, Loss: %f' % (e, round(error.mean(), 4)))\n",
    "                print(f\"Accuracy: {round(pred.mean() * 100, 4)}%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T03:34:50.676153Z",
     "start_time": "2020-05-25T03:34:50.515245Z"
    },
    "code_folding": [
     1,
     2,
     22,
     56,
     96,
     119,
     141,
     162,
     178,
     200,
     208,
     247,
     268,
     303,
     314,
     354,
     379,
     383
    ]
   },
   "outputs": [],
   "source": [
    "# this class is great (old)\n",
    "class FFL():\n",
    "    \"\"\"\n",
    "        A class to create a single input/hidden feedforward layer of given shape.\n",
    "        \n",
    "        Input:\n",
    "        -------\n",
    "        n_input: Number of input values to layer. If none, tried to check from preious layer.\n",
    "        neurons: Neurons on that layer.\n",
    "        bias: External bias numpy array. If None, used from np.random.randn(neurons)\n",
    "        weights: External weight numpy array. If None, used from np.random.randn(n_input, neurons)\n",
    "        activation: One of [\"relu\", \"sigmoid\", \"tanh\", \"softmax\"].\n",
    "        is_bias: Do we want bias to be used here? Default True.\n",
    "        \n",
    "        Output:\n",
    "        --------\n",
    "        Object of FFL which can be stacked later to use.\n",
    "        \n",
    "        Example:\n",
    "        FFL(2, 2, activation=\"sigmoid\")\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_input=None, neurons=1, bias=None, weights=None, activation=None, is_bias = True):\n",
    "        \n",
    "        np.random.seed(100)\n",
    "        self.n_input = n_input\n",
    "        self.neurons = neurons\n",
    "        self.isbias = is_bias\n",
    "        self.name = \"\"\n",
    "        \n",
    "        if self.n_input != None:\n",
    "            self.weights = weights if weights != None else np.random.normal(size = (n_input, neurons))\n",
    "            \n",
    "        if(is_bias):\n",
    "            self.biases = bias if bias != None else np.random.normal(size=(neurons))\n",
    "        else:\n",
    "            self.biases = 0\n",
    "            \n",
    "        self.out = None\n",
    "        self.input = None\n",
    "        self.error = None\n",
    "        self.delta = None\n",
    "        activations = [\"relu\", \"sigmoid\", \"tanh\", \"softmax\"]\n",
    "        self.delta_weights = 0\n",
    "        self.delta_biases = 0\n",
    "        self.pdelta_weights = 0\n",
    "        self.pdelta_biases = 0\n",
    "        self.dprev = [0, 0]\n",
    "        #self.bias = np.ones(self.bias.shape)\n",
    "        \n",
    "        if activation not in activations and activation != None:\n",
    "             raise ValueError(f\"Activation function not recognised. Use one of {activations} instead.\")\n",
    "        else:\n",
    "            self.activation = activation\n",
    "            \n",
    "           \n",
    "    def activation_dfn(self, r):\n",
    "        \"\"\"\n",
    "            A method of FFL to find derivative of given activation function.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        if self.activation is None:\n",
    "            return np.ones(r.shape)\n",
    "\n",
    "        if self.activation == 'tanh':\n",
    "            return 1 - r ** 2\n",
    "\n",
    "        if self.activation == 'sigmoid':\n",
    "            return r * (1 - r)\n",
    "\n",
    "        if self.activation == \"softmax\":\n",
    "            soft = self.activation_fn(r)\n",
    "                    \n",
    "            #s = soft.reshape(-1, 1)\n",
    "            #dsoft = np.diagflat(s) - np.dot(s, s.T)\n",
    "            #diag_soft = dsoft.diagonal()\n",
    "            \n",
    "            # take only the diagonal of dsoft i.e i==j only\n",
    "            \"\"\"\n",
    "                soft = a / np.sum(a)\n",
    "\n",
    "                dsoft = np.diag(soft)\n",
    "                for i in range(len(x)):\n",
    "                    for j in range(len(soft)):\n",
    "                        if i == j:\n",
    "                            d = 1\n",
    "                        else:\n",
    "                            d = 0\n",
    "                        dsoft[i][j] = soft[i] * (d - soft[j])\n",
    "            \"\"\"\n",
    "            diag_soft = soft*(1- soft)\n",
    "            return diag_soft\n",
    "        \n",
    "        return r\n",
    "\n",
    "    def activation_fn(self, r):\n",
    "        \"\"\"\n",
    "        A method of FFL which contains the operation and defination of given activation function.\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.activation == None or self.activation == \"linear\":\n",
    "            return r\n",
    "\n",
    "        # tanh\n",
    "        if self.activation == 'tanh':\n",
    "            return np.tanh(r)\n",
    "\n",
    "        # sigmoid\n",
    "        if self.activation == 'sigmoid':\n",
    "            \n",
    "            return 1 / (1 + np.exp(-r))\n",
    "\n",
    "        if self.activation == \"softmax\":\n",
    "            # stable softmax\n",
    "            r = r - np.max(r)\n",
    "            s = np.exp(r)\n",
    "            return s / np.sum(s)\n",
    "        \n",
    "    def apply_activation(self, x):\n",
    "        soma = np.dot(x, self.weights) + self.biases\n",
    "        self.out = self.activation_fn(soma)\n",
    "        \n",
    "        return self.out\n",
    "    def set_n_input(self):\n",
    "        self.weights = self.w if self.w != None else np.random.normal(size=(self.n_input, self.neurons))\n",
    "    def backpropagate(self, nx_layer):\n",
    "        self.error = np.dot(nx_layer.weights, nx_layer.delta)\n",
    "        self.delta = self.error * self.activation_dfn(self.out)\n",
    "        self.delta_weights += self.delta * np.atleast_2d(self.input).T\n",
    "        self.delta_biases += self.delta\n",
    "    \n",
    "    # added below methods from cnn\n",
    "    def set_output_shape(self):\n",
    "        self.set_n_input()\n",
    "        self.get_parameters()\n",
    "        self.output_shape = self.neurons\n",
    "    def get_parameters(self):\n",
    "        self.parameters = self.input_shape *  self.neurons + self.neurons if self.isbias else 0  \n",
    "        return self.parameters\n",
    "# old NN\n",
    "class NN:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.info_df = {}\n",
    "        self.column = [\"LName\", \"Input\", \"Output\", \"Activation\", \"Bias\"]\n",
    "        self.parameters = 0\n",
    "        self.optimizer = \"\"\n",
    "        self.loss = \"\"\n",
    "        self.all_loss = {}\n",
    "        self.lr = 1\n",
    "        self.metrics = []\n",
    "        self.av_optimizers = [\"sgd\", \"iterative\", \"momentum\", \"rmsprop\", \"adagrad\", \"adam\", \"adamax\", \"adadelta\"]\n",
    "        self.av_metrics = [\"mse\", \"accuracy\", \"cse\"]\n",
    "        self.av_loss = [\"mse\", \"cse\"]\n",
    "        self.iscompiled = False\n",
    "    \n",
    "        self.batch_size = 8\n",
    "        self.mr = 0.0001\n",
    "        self.all_acc = []\n",
    "        self.eps = 1e-8\n",
    "    \n",
    "    def add(self, layer):\n",
    "        if(len(self.layers) > 0):\n",
    "            prev_layer = self.layers[-1]\n",
    "            if prev_layer.name != \"Input Layer\":\n",
    "                prev_layer.name = f\"Hidden Layer{len(self.layers) - 1}\" \n",
    "            if layer.n_input == None:\n",
    "                layer.n_input = prev_layer.neurons\n",
    "                layer.set_n_input()\n",
    "            layer.name = \"Output Layer\"\n",
    "            if prev_layer.neurons != layer.n_input and layer.n_input != None:\n",
    "                raise ValueError(f\"This layer '{layer.name}' must have neurons={prev_layer.neurons} because '{prev_layer.name}' has output of {prev_layer.neurons}.\")\n",
    "        else:\n",
    "            layer.name = \"Input Layer\"\n",
    "        layer.parameters = layer.n_input * layer.neurons + layer.neurons * 1 if layer.isbias else 0\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    def summary(self):\n",
    "        lname = []\n",
    "        linput = []\n",
    "        lneurons = []\n",
    "        lactivation = []\n",
    "        lisbias = []\n",
    "        for layer in self.layers:\n",
    "            #self.layer_info.append(f\"\\t|| {layer.name} \\t || {layer.n_input} \\t || {layer.neurons} \\t|| {layer.activation} \\t || {layer.isbias} ||\\n\") \n",
    "            lname.append(layer.name)\n",
    "            linput.append(layer.n_input)\n",
    "            lneurons.append(layer.neurons)\n",
    "            lactivation.append(layer.activation)\n",
    "            lisbias.append(layer.isbias)\n",
    "            layer.parameters = layer.n_input *  layer.neurons + layer.neurons if layer.isbias else 0\n",
    "            self.parameters += layer.parameters\n",
    "        model_dict = {\"Layer Name\": lname, \"Input\": linput, \"Neurons\": lneurons, \"Activation\": lactivation, \"Bias\": lisbias}    \n",
    "        model_df = pd.DataFrame(model_dict).set_index(\"Layer Name\")\n",
    "        #summ = \"\".join(self.layer_info)\n",
    "        #print(summ)\n",
    "        print(model_df)\n",
    "        print(\"Total Parameters: \", self.parameters)\n",
    "    \n",
    "    def visualize(self):\n",
    "        k = list(self.all_loss.keys())\n",
    "        v = list(self.all_loss.values())\n",
    "        plt.plot(k, v)\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(self.loss)\n",
    "        plt.show()\n",
    "    \n",
    "    def compile_model(self, lr=0.01, mr = 0.001, opt = \"sgd\", loss = \"mse\", metrics=['mse']):\n",
    "        if opt not in self.av_optimizers:\n",
    "            raise ValueError(f\"Optimizer is not understood, use one of {self.av_optimizers}.\")\n",
    "        \n",
    "        for m in metrics:\n",
    "            if m not in self.av_metrics:\n",
    "                raise ValueError(f\"Metrics is not understood, use one of {self.av_metrics}.\")\n",
    "        \n",
    "        if loss not in self.av_loss:\n",
    "            raise ValueError(f\"Loss function is not understood, use one of {self.av_loss}.\")\n",
    "        \n",
    "        #self.optimizer = opt\n",
    "        self.loss = loss\n",
    "        self.lr = lr\n",
    "        self.mr = mr\n",
    "        self.metrics = metrics\n",
    "        self.iscompiled = True\n",
    "        self.optimizer = Optimizer(layers=self.layers, name=opt, learning_rate=lr, mr=mr)\n",
    "        self.optimizer = self.optimizer.opt_dict[opt]\n",
    "        #print(self.optimizer)\n",
    "        \"\"\"\n",
    "            if opt == \"sgd\":\n",
    "                self.optimizer = Optimizer(layers=self.layers, name=\"sgd\", learning_rate=lr)\n",
    "                self.optimizer = self.optimizer.sgd\n",
    "            elif opt == \"adam\":\n",
    "                self.optimizer = Optimizer(layers=self.layers, name=\"adam\", learning_rate=lr)\n",
    "                self.optimizer = self.optimizer.adam\n",
    "                self.optimizer = self.ADAM\n",
    "                alpha = self.lr\n",
    "                beta1 = 0.9\n",
    "                beta2 = 0.999\n",
    "                epsilon = 1e-8\n",
    "                for l in self.layers:\n",
    "                    l.adam_param = []\n",
    "                    # w, a, b1, b2, eps, m, v, t\n",
    "                    l.adam_param = [[l.weights, alpha, beta1, beta2, epsilon, 0, 0, 0]]\n",
    "                    if l.isbias:\n",
    "                        l.adam_param.append([l.bias, alpha, beta1, beta2, epsilon, 0, 0, 0])\n",
    "        \"\"\"\n",
    "    def SGD(self, update):\n",
    "        for i in range(len(self.layers)):\n",
    "            layer = self.layers[i]       \n",
    "            #print(layer.delta.shape, layer.delta_weights.shape, layer.pdelta_biases)\n",
    "            input_to_use = np.atleast_2d(layer.input)\n",
    "            #dw = layer.delta * input_to_use[-1].T \n",
    "            #db =  layer.delta \n",
    "            if update:\n",
    "                layer.delta_weights /= self.batch_size\n",
    "                layer.delta_biases /= self.batch_size\n",
    "                #print(layer.delta_weights.shape, layer.weights.shape)\n",
    "                layer.weights += (layer.pdelta_weights * self.mr  + layer.delta_weights * self.lr)\n",
    "                layer.biases += (layer.pdelta_biases * self.mr  +  layer.delta_biases * self.lr)\n",
    "                layer.pdelta_weights = layer.delta_weights\n",
    "                layer.pdelta_biases = layer.delta_biases\n",
    "            #else:\n",
    "                layer.delta_weights = 0\n",
    "                layer.delta_biases = 0 \n",
    "            \n",
    "            \n",
    "     \n",
    "    def ADAM(self, update):\n",
    "        \n",
    "        for i in range(len(self.layers)):\n",
    "            layer = self.layers[i]         \n",
    "            input_to_use = np.atleast_2d(layer.input)\n",
    "            dw = layer.delta * input_to_use.T \n",
    "            db =  layer.delta \n",
    "            dparam = [dw, db]\n",
    "            nparam = []\n",
    "            l = layer\n",
    "            for i, grad in enumerate(dparam):\n",
    "                #print(i)\n",
    "                theta = l.adam_param[i][0]\n",
    "                alpha = l.adam_param[i][1]\n",
    "                beta1 = l.adam_param[i][2]\n",
    "                beta2 = l.adam_param[i][3]\n",
    "                epsilon = l.adam_param[i][4]\n",
    "                m = l.adam_param[i][5]\n",
    "                v = l.adam_param[i][6]\n",
    "                t = l.adam_param[i][7] + 1\n",
    "                             \n",
    "                m = beta1 * m + (1-beta1) * grad # moving avg of grad\n",
    "                v = beta2 * v + (1-beta2) * np.power(grad, 2) #\n",
    "                m_cap = m / (1 - np.power(beta1, t))\n",
    "                v_cap = v / (1 - np.power(beta2, t))\n",
    "                theta += (self.mr * layer.dprev[i]) + (alpha * np.nan_to_num(m_cap / (np.sqrt(v_cap) + epsilon)))\n",
    "                layer.dprev[i] = np.nan_to_num((m_cap / (np.sqrt(v_cap) + epsilon))) * 0\n",
    "                l.adam_param[i] = [theta, alpha, beta1, beta2, epsilon, m, v, t]\n",
    "                nparam.append(theta)\n",
    "            if len(nparam) == 2:\n",
    "                layer.weights = nparam[0]\n",
    "                layer.bias = nparam[1]\n",
    "            else:\n",
    "                layer.weights = nparam[0]\n",
    "            \n",
    "    def feedforward(self, x):\n",
    "        for l in self.layers:\n",
    "            l.input = x\n",
    "            \n",
    "            x = l.apply_activation(x)  \n",
    "            #print(l.name, \"ip\", l.input.shape, \"op\", x.shape)\n",
    "            #print(l.weights.shape, l.bias)\n",
    "            l.out = x\n",
    "            \n",
    "        return x\n",
    "        \n",
    "    def predict(self, X):\n",
    "        out = []\n",
    "        for x in X:\n",
    "            out.append(self.feedforward(x))\n",
    "        return out\n",
    "#         pass\n",
    "    def apply_loss(self, y, out):\n",
    "        if self.loss == \"mse\":\n",
    "            loss = y - out\n",
    "            mse = np.mean(np.square(loss))\n",
    "#             self.all_loss.append(mse)          \n",
    "            return loss, mse\n",
    "        if self.loss == 'cse':\n",
    "            \"\"\" Requires out to be probability values. \"\"\"     \n",
    "            if len(out) == len(y) == 1:\n",
    "                #print(\"Using Binary CSE.\")\n",
    "                #y += self.eps\n",
    "                #out += self.eps\n",
    "                cse = -(y * np.log(out) + (1 - y) * np.log(1 - out))\n",
    "                loss = -(y / out - (1 - y) / (1 - out))\n",
    "                #cse = np.mean(abs(cse))\n",
    "            else:\n",
    "                #print(\"Using Categorical CSE.\")\n",
    "                if self.layers[-1].activation == \"softmax\":\n",
    "                    # if o/p layer's fxn is softmax then loss is y - out\n",
    "                    # check the derivation of softmax and crossentropy with derivative\n",
    "                    loss = y - out\n",
    "                    loss = np.nan_to_num(loss / self.layers[-1].activation_dfn(out))\n",
    "                else:\n",
    "                    y = np.float64(y)\n",
    "                    #y += self.eps\n",
    "                    out += self.eps\n",
    "                    #cse =  -np.sum(y * (np.log(out)))\n",
    "                    \n",
    "                    loss = -(np.nan_to_num(y / out, posinf=0, neginf=0) - np.nan_to_num((1 - y) / (1 - out), posinf=0, neginf=0))\n",
    "                \n",
    "            \n",
    "                cse = -np.sum(y * np.nan_to_num(np.log(out), posinf=0, neginf=0) + (1 - y) * np.nan_to_num(np.log(1 - out), posinf=0, neginf=0))\n",
    "            return loss, cse\n",
    "        \n",
    "    def backpropagate(self, loss, update = True):\n",
    "        \n",
    "        \n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            layer = self.layers[i]\n",
    "            if layer == self.layers[-1]:\n",
    "                layer.error = loss\n",
    "                \n",
    "                layer.delta = layer.error * layer.activation_dfn(layer.out) \n",
    "                #layer.pdelta_weights = layer.delta_weights\n",
    "                #layer.pdelta_biases = layer.delta_biases\n",
    "\n",
    "                layer.delta_weights += layer.delta * np.atleast_2d(layer.input).T\n",
    "                layer.delta_biases += layer.delta\n",
    "\n",
    "            else:\n",
    "                nx_layer = self.layers[i+1]\n",
    "                layer.backpropagate(nx_layer)\n",
    "            if update:\n",
    "                layer.delta_weights /= self.batch_size\n",
    "                layer.delta_biases /= self.batch_size\n",
    "        if update:\n",
    "            #self.SGD(update)       \n",
    "            self.optimizer(self.layers)\n",
    "            self.zerograd()\n",
    "    def zerograd(self):\n",
    "        for l in self.layers:\n",
    "            l.delta_weights=0\n",
    "            l.delta_biases = 0\n",
    "    def check_trainnable(self, X, Y):\n",
    "        if self.iscompiled == False:\n",
    "            raise ValueError(\"Model is not compiled.\")\n",
    "        if len(X) != len(Y):\n",
    "            raise ValueError(\"Length of training input and label is not equal.\")\n",
    "        if X[0].shape[0] != self.layers[0].n_input:\n",
    "            layer = self.layers[0]\n",
    "            raise ValueError(f\"'{layer.name}' expects input of {layer.n_input} while {X[0].shape[0]} is given.\")\n",
    "        if Y.shape[-1] != self.layers[-1].neurons:\n",
    "            op_layer = self.layers[-1]\n",
    "            raise ValueError(f\"'{op_layer.name}' expects input of {op_layer.neurons} while {Y.shape[-1]} is given.\")  \n",
    "        \n",
    "    def train(self, X, Y, epochs, show_every=1, batch_size = 32, shuffle=True):\n",
    "        self.check_trainnable(X, Y)\n",
    "        self.batch_size = batch_size\n",
    "        t1 = time.time()\n",
    "        self.losses = []\n",
    "        \n",
    "        len_batch = int(len(X)/batch_size)\n",
    "        \n",
    "        \n",
    "        batches = []\n",
    "        \n",
    "        curr_ind = np.arange(0, len(X), dtype=np.int32)\n",
    "        if shuffle: \n",
    "            np.random.shuffle(curr_ind)\n",
    "\n",
    "        if(len(curr_ind) % batch_size) != 0 :\n",
    "            nx = batch_size - len(curr_ind) % batch_size\n",
    "            nx = curr_ind[:nx]\n",
    "            curr_ind = np.hstack([curr_ind, nx])\n",
    "        #print(curr_ind.shape, len_batch)\n",
    "        batches = np.split(curr_ind, batch_size)\n",
    "\n",
    "        #print(len(batches))\n",
    "        for e in range(epochs):            \n",
    "            \n",
    "            err = []\n",
    "            for batch in batches:\n",
    "                a = [] \n",
    "                curr_x, curr_y = X[batch], Y[batch]\n",
    "                b = 0\n",
    "                batch_loss = 0\n",
    "                for x, y in zip(curr_x, curr_y):\n",
    "                    out = self.feedforward(x)\n",
    "                    loss, error = self.apply_loss(y, out)\n",
    "                    #loss = loss.mean(axis=0)\n",
    "                    batch_loss += loss\n",
    "                    err.append(error)\n",
    "                    update = False\n",
    "                    if b == batch_size-1:\n",
    "                        update = True\n",
    "                        loss = batch_loss/batch_size\n",
    "                    self.backpropagate(loss, update)\n",
    "                    b+=1\n",
    "                \n",
    "\n",
    "            if e % show_every == 0:      \n",
    "                out = self.feedforward(X)\n",
    "                loss, error = self.apply_loss(Y, out)\n",
    "                #print(error)\n",
    "                out_activation = self.layers[-1].activation\n",
    "                print(out_activation)\n",
    "                if out_activation == \"softmax\":\n",
    "                    #print(out, Y)\n",
    "                    pred = out.argmax(axis=1) == Y.argmax(axis=1)\n",
    "                    #loss, error = self.apply_loss(Y, out)\n",
    "                elif out_activation == \"sigmoid\":\n",
    "                    pred = out > 0.7\n",
    "                    #pred = pred == Y\n",
    "                elif out_activation == None:\n",
    "                    pred = abs(Y-out) < 0.000001\n",
    "                    \n",
    "                self.all_loss[e] = loss\n",
    "                a = np.array(a)\n",
    "                err = np.array(err)\n",
    "                print(f\"Time: {round(time.time() - t1, 3)}sec\")\n",
    "                t1 = time.time()\n",
    "                print('Epoch: #%s, Loss: %f' % (e, round(error.mean(), 4)))\n",
    "                print(f\"Accuracy: {round(pred.mean() * 100, 4)}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T06:04:21.028744Z",
     "start_time": "2020-05-26T04:38:45.573164Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Input  Neurons Activation  Bias\n",
      "Layer Name                                   \n",
      "Input Layer     784      100    sigmoid  True\n",
      "Output Layer    100       10    softmax  True\n",
      "Total Parameters:  79510\n",
      "softmax\n",
      "Time: 49.407sec\n",
      "Epoch: #0, Loss: 1225237.775500\n",
      "Accuracy: 24.195%\n",
      "softmax\n",
      "Time: 47.948sec\n",
      "Epoch: #1, Loss: 1121328.000400\n",
      "Accuracy: 40.0733%\n",
      "softmax\n",
      "Time: 48.503sec\n",
      "Epoch: #2, Loss: 1062563.824800\n",
      "Accuracy: 50.995%\n",
      "softmax\n",
      "Time: 52.588sec\n",
      "Epoch: #3, Loss: 1012074.097600\n",
      "Accuracy: 58.5667%\n",
      "softmax\n",
      "Time: 62.421sec\n",
      "Epoch: #4, Loss: 1011690.071300\n",
      "Accuracy: 63.9533%\n",
      "softmax\n",
      "Time: 53.218sec\n",
      "Epoch: #5, Loss: 1027151.735000\n",
      "Accuracy: 67.7767%\n",
      "softmax\n",
      "Time: 51.171sec\n",
      "Epoch: #6, Loss: 1031567.850100\n",
      "Accuracy: 70.7867%\n",
      "softmax\n",
      "Time: 50.271sec\n",
      "Epoch: #7, Loss: 1030046.386300\n",
      "Accuracy: 73.1917%\n",
      "softmax\n",
      "Time: 50.46sec\n",
      "Epoch: #8, Loss: 1029917.048000\n",
      "Accuracy: 75.0317%\n",
      "softmax\n",
      "Time: 50.75sec\n",
      "Epoch: #9, Loss: 1032357.474800\n",
      "Accuracy: 76.57%\n",
      "softmax\n",
      "Time: 50.939sec\n",
      "Epoch: #10, Loss: 1035044.798200\n",
      "Accuracy: 78.015%\n",
      "softmax\n",
      "Time: 50.696sec\n",
      "Epoch: #11, Loss: 1037056.796600\n",
      "Accuracy: 79.205%\n",
      "softmax\n",
      "Time: 55.579sec\n",
      "Epoch: #12, Loss: 1036343.830600\n",
      "Accuracy: 80.2617%\n",
      "softmax\n",
      "Time: 62.431sec\n",
      "Epoch: #13, Loss: 1034256.996300\n",
      "Accuracy: 81.1267%\n",
      "softmax\n",
      "Time: 50.841sec\n",
      "Epoch: #14, Loss: 1031434.458900\n",
      "Accuracy: 81.99%\n",
      "softmax\n",
      "Time: 49.438sec\n",
      "Epoch: #15, Loss: 1027899.361700\n",
      "Accuracy: 82.7283%\n",
      "softmax\n",
      "Time: 50.796sec\n",
      "Epoch: #16, Loss: 1024594.705700\n",
      "Accuracy: 83.3083%\n",
      "softmax\n",
      "Time: 50.452sec\n",
      "Epoch: #17, Loss: 1022542.210600\n",
      "Accuracy: 83.885%\n",
      "softmax\n",
      "Time: 52.598sec\n",
      "Epoch: #18, Loss: 1021778.588500\n",
      "Accuracy: 84.4267%\n",
      "softmax\n",
      "Time: 52.769sec\n",
      "Epoch: #19, Loss: 1022054.270100\n",
      "Accuracy: 84.9167%\n",
      "softmax\n",
      "Time: 51.835sec\n",
      "Epoch: #20, Loss: 1022918.684000\n",
      "Accuracy: 85.355%\n",
      "softmax\n",
      "Time: 49.842sec\n",
      "Epoch: #21, Loss: 1023901.356700\n",
      "Accuracy: 85.755%\n",
      "softmax\n",
      "Time: 51.791sec\n",
      "Epoch: #22, Loss: 1024827.276400\n",
      "Accuracy: 86.1533%\n",
      "softmax\n",
      "Time: 51.618sec\n",
      "Epoch: #23, Loss: 1025698.915300\n",
      "Accuracy: 86.5533%\n",
      "softmax\n",
      "Time: 50.773sec\n",
      "Epoch: #24, Loss: 1026509.647000\n",
      "Accuracy: 86.9167%\n",
      "softmax\n",
      "Time: 48.391sec\n",
      "Epoch: #25, Loss: 1027249.635800\n",
      "Accuracy: 87.25%\n",
      "softmax\n",
      "Time: 47.291sec\n",
      "Epoch: #26, Loss: 1027889.966300\n",
      "Accuracy: 87.54%\n",
      "softmax\n",
      "Time: 51.254sec\n",
      "Epoch: #27, Loss: 1028409.724500\n",
      "Accuracy: 87.82%\n",
      "softmax\n",
      "Time: 48.414sec\n",
      "Epoch: #28, Loss: 1028814.831500\n",
      "Accuracy: 88.1%\n",
      "softmax\n",
      "Time: 47.653sec\n",
      "Epoch: #29, Loss: 1029121.081200\n",
      "Accuracy: 88.3633%\n",
      "softmax\n",
      "Time: 50.174sec\n",
      "Epoch: #30, Loss: 1029338.674700\n",
      "Accuracy: 88.62%\n",
      "softmax\n",
      "Time: 51.04sec\n",
      "Epoch: #31, Loss: 1029471.800500\n",
      "Accuracy: 88.8483%\n",
      "softmax\n",
      "Time: 49.106sec\n",
      "Epoch: #32, Loss: 1029560.036100\n",
      "Accuracy: 89.0833%\n",
      "softmax\n",
      "Time: 52.338sec\n",
      "Epoch: #33, Loss: 1029639.920600\n",
      "Accuracy: 89.3217%\n",
      "softmax\n",
      "Time: 51.083sec\n",
      "Epoch: #34, Loss: 1029674.639700\n",
      "Accuracy: 89.5333%\n",
      "softmax\n",
      "Time: 50.899sec\n",
      "Epoch: #35, Loss: 1029587.372800\n",
      "Accuracy: 89.7967%\n",
      "softmax\n",
      "Time: 52.719sec\n",
      "Epoch: #36, Loss: 1029355.478300\n",
      "Accuracy: 89.9917%\n",
      "softmax\n",
      "Time: 52.405sec\n",
      "Epoch: #37, Loss: 1029019.297200\n",
      "Accuracy: 90.1783%\n",
      "softmax\n",
      "Time: 52.791sec\n",
      "Epoch: #38, Loss: 1028625.868500\n",
      "Accuracy: 90.385%\n",
      "softmax\n",
      "Time: 53.366sec\n",
      "Epoch: #39, Loss: 1028195.122800\n",
      "Accuracy: 90.5367%\n",
      "softmax\n",
      "Time: 57.17sec\n",
      "Epoch: #40, Loss: 1027711.933500\n",
      "Accuracy: 90.73%\n",
      "softmax\n",
      "Time: 51.272sec\n",
      "Epoch: #41, Loss: 1027123.907500\n",
      "Accuracy: 90.8733%\n",
      "softmax\n",
      "Time: 51.131sec\n",
      "Epoch: #42, Loss: 1026402.343200\n",
      "Accuracy: 91.0483%\n",
      "softmax\n",
      "Time: 51.466sec\n",
      "Epoch: #43, Loss: 1025572.191400\n",
      "Accuracy: 91.21%\n",
      "softmax\n",
      "Time: 47.902sec\n",
      "Epoch: #44, Loss: 1024671.350000\n",
      "Accuracy: 91.3633%\n",
      "softmax\n",
      "Time: 49.535sec\n",
      "Epoch: #45, Loss: 1023730.232400\n",
      "Accuracy: 91.4967%\n",
      "softmax\n",
      "Time: 50.687sec\n",
      "Epoch: #46, Loss: 1022782.899700\n",
      "Accuracy: 91.635%\n",
      "softmax\n",
      "Time: 48.181sec\n",
      "Epoch: #47, Loss: 1021861.703800\n",
      "Accuracy: 91.8017%\n",
      "softmax\n",
      "Time: 50.619sec\n",
      "Epoch: #48, Loss: 1020956.982100\n",
      "Accuracy: 91.9467%\n",
      "softmax\n",
      "Time: 50.45sec\n",
      "Epoch: #49, Loss: 1020053.958600\n",
      "Accuracy: 92.0967%\n",
      "softmax\n",
      "Time: 49.784sec\n",
      "Epoch: #50, Loss: 1019147.512500\n",
      "Accuracy: 92.2067%\n",
      "softmax\n",
      "Time: 53.274sec\n",
      "Epoch: #51, Loss: 1018242.489100\n",
      "Accuracy: 92.3483%\n",
      "softmax\n",
      "Time: 53.638sec\n",
      "Epoch: #52, Loss: 1017367.327300\n",
      "Accuracy: 92.4967%\n",
      "softmax\n",
      "Time: 53.465sec\n",
      "Epoch: #53, Loss: 1016560.073600\n",
      "Accuracy: 92.5983%\n",
      "softmax\n",
      "Time: 52.405sec\n",
      "Epoch: #54, Loss: 1015826.609300\n",
      "Accuracy: 92.73%\n",
      "softmax\n",
      "Time: 53.199sec\n",
      "Epoch: #55, Loss: 1015149.492800\n",
      "Accuracy: 92.8367%\n",
      "softmax\n",
      "Time: 49.8sec\n",
      "Epoch: #56, Loss: 1014497.475000\n",
      "Accuracy: 92.9617%\n",
      "softmax\n",
      "Time: 49.483sec\n",
      "Epoch: #57, Loss: 1013842.629500\n",
      "Accuracy: 93.05%\n",
      "softmax\n",
      "Time: 50.689sec\n",
      "Epoch: #58, Loss: 1013175.232300\n",
      "Accuracy: 93.1583%\n",
      "softmax\n",
      "Time: 48.208sec\n",
      "Epoch: #59, Loss: 1012505.444800\n",
      "Accuracy: 93.2733%\n",
      "softmax\n",
      "Time: 48.188sec\n",
      "Epoch: #60, Loss: 1011842.860500\n",
      "Accuracy: 93.3733%\n",
      "softmax\n",
      "Time: 50.42sec\n",
      "Epoch: #61, Loss: 1011190.100900\n",
      "Accuracy: 93.4533%\n",
      "softmax\n",
      "Time: 48.143sec\n",
      "Epoch: #62, Loss: 1010544.930700\n",
      "Accuracy: 93.5083%\n",
      "softmax\n",
      "Time: 49.306sec\n",
      "Epoch: #63, Loss: 1009903.075900\n",
      "Accuracy: 93.6067%\n",
      "softmax\n",
      "Time: 48.835sec\n",
      "Epoch: #64, Loss: 1009263.300400\n",
      "Accuracy: 93.6967%\n",
      "softmax\n",
      "Time: 50.567sec\n",
      "Epoch: #65, Loss: 1008628.180600\n",
      "Accuracy: 93.7867%\n",
      "softmax\n",
      "Time: 53.724sec\n",
      "Epoch: #66, Loss: 1008000.154300\n",
      "Accuracy: 93.865%\n",
      "softmax\n",
      "Time: 49.442sec\n",
      "Epoch: #67, Loss: 1007377.384400\n",
      "Accuracy: 93.93%\n",
      "softmax\n",
      "Time: 49.19sec\n",
      "Epoch: #68, Loss: 1006755.834200\n",
      "Accuracy: 94.0083%\n",
      "softmax\n",
      "Time: 49.63sec\n",
      "Epoch: #69, Loss: 1006129.885000\n",
      "Accuracy: 94.0783%\n",
      "softmax\n",
      "Time: 48.105sec\n",
      "Epoch: #70, Loss: 1005488.105100\n",
      "Accuracy: 94.1367%\n",
      "softmax\n",
      "Time: 47.753sec\n",
      "Epoch: #71, Loss: 1004828.434800\n",
      "Accuracy: 94.2167%\n",
      "softmax\n",
      "Time: 49.501sec\n",
      "Epoch: #72, Loss: 1004172.844400\n",
      "Accuracy: 94.3167%\n",
      "softmax\n",
      "Time: 54.34sec\n",
      "Epoch: #73, Loss: 1003551.080100\n",
      "Accuracy: 94.3867%\n",
      "softmax\n",
      "Time: 51.995sec\n",
      "Epoch: #74, Loss: 1002975.773900\n",
      "Accuracy: 94.4483%\n",
      "softmax\n",
      "Time: 50.788sec\n",
      "Epoch: #75, Loss: 1002442.842800\n",
      "Accuracy: 94.5167%\n",
      "softmax\n",
      "Time: 48.721sec\n",
      "Epoch: #76, Loss: 1001933.549600\n",
      "Accuracy: 94.6133%\n",
      "softmax\n",
      "Time: 50.519sec\n",
      "Epoch: #77, Loss: 1001432.014700\n",
      "Accuracy: 94.6767%\n",
      "softmax\n",
      "Time: 52.209sec\n",
      "Epoch: #78, Loss: 1000952.009900\n",
      "Accuracy: 94.75%\n",
      "softmax\n",
      "Time: 51.449sec\n",
      "Epoch: #79, Loss: 1000504.457500\n",
      "Accuracy: 94.82%\n",
      "softmax\n",
      "Time: 57.457sec\n",
      "Epoch: #80, Loss: 1000080.742200\n",
      "Accuracy: 94.89%\n",
      "softmax\n",
      "Time: 53.626sec\n",
      "Epoch: #81, Loss: 999669.598000\n",
      "Accuracy: 94.9717%\n",
      "softmax\n",
      "Time: 65.617sec\n",
      "Epoch: #82, Loss: 999258.780200\n",
      "Accuracy: 95.02%\n",
      "softmax\n",
      "Time: 55.132sec\n",
      "Epoch: #83, Loss: 998848.425000\n",
      "Accuracy: 95.095%\n",
      "softmax\n",
      "Time: 54.064sec\n",
      "Epoch: #84, Loss: 998443.511100\n",
      "Accuracy: 95.1617%\n",
      "softmax\n",
      "Time: 51.375sec\n",
      "Epoch: #85, Loss: 998043.762500\n",
      "Accuracy: 95.2267%\n",
      "softmax\n",
      "Time: 50.172sec\n",
      "Epoch: #86, Loss: 997644.282900\n",
      "Accuracy: 95.2733%\n",
      "softmax\n",
      "Time: 49.041sec\n",
      "Epoch: #87, Loss: 997242.859100\n",
      "Accuracy: 95.3217%\n",
      "softmax\n",
      "Time: 50.583sec\n",
      "Epoch: #88, Loss: 996837.819300\n",
      "Accuracy: 95.3867%\n",
      "softmax\n",
      "Time: 51.104sec\n",
      "Epoch: #89, Loss: 996426.008500\n",
      "Accuracy: 95.4283%\n",
      "softmax\n",
      "Time: 51.659sec\n",
      "Epoch: #90, Loss: 996014.466700\n",
      "Accuracy: 95.5%\n",
      "softmax\n",
      "Time: 49.301sec\n",
      "Epoch: #91, Loss: 995618.442500\n",
      "Accuracy: 95.5583%\n",
      "softmax\n",
      "Time: 51.814sec\n",
      "Epoch: #92, Loss: 995249.514400\n",
      "Accuracy: 95.6067%\n",
      "softmax\n",
      "Time: 51.573sec\n",
      "Epoch: #93, Loss: 994905.992800\n",
      "Accuracy: 95.6633%\n",
      "softmax\n",
      "Time: 51.144sec\n",
      "Epoch: #94, Loss: 994574.971600\n",
      "Accuracy: 95.7217%\n",
      "softmax\n",
      "Time: 51.575sec\n",
      "Epoch: #95, Loss: 994251.763700\n",
      "Accuracy: 95.76%\n",
      "softmax\n",
      "Time: 51.078sec\n",
      "Epoch: #96, Loss: 993941.201300\n",
      "Accuracy: 95.8067%\n",
      "softmax\n",
      "Time: 49.636sec\n",
      "Epoch: #97, Loss: 993662.587700\n",
      "Accuracy: 95.87%\n",
      "softmax\n",
      "Time: 51.408sec\n",
      "Epoch: #98, Loss: 993442.829200\n",
      "Accuracy: 95.9117%\n",
      "softmax\n",
      "Time: 51.575sec\n",
      "Epoch: #99, Loss: 993308.674700\n",
      "Accuracy: 95.9517%\n"
     ]
    }
   ],
   "source": [
    "# from keras.datasets import mnist\n",
    "# (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x = x_train.reshape(-1, 28 * 28) \n",
    "x = (x-x.mean(axis=1).reshape(-1, 1))/x.std(axis=1).reshape(-1, 1)\n",
    "y = pd.get_dummies(y_train).to_numpy()\n",
    "\n",
    "m = NN()\n",
    "m.add(FFL(784, 100, activation=\"sigmoid\")) \n",
    "m.add(FFL(100, 10, activation=\"softmax\")) \n",
    "# every opt works with cse but i always have to sum the delta weight why? \n",
    "# mse wont work with any opt why?\n",
    "# use non linear activation fxn to solve 1st problem\n",
    "m.compile_model(lr=0.01, opt=\"adam\", loss=\"cse\", mr= 0.001)\n",
    "m.summary()\n",
    "m.train(x[:], y[:], epochs=100, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T06:35:51.146940Z",
     "start_time": "2020-05-26T06:35:50.509304Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFgAAAEICAYAAADbbDN3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOy9eZRkyVXf/4m3575V1r53dXV3dff0Mvs+0zOa0UiypJ8khBCL9RMG28ABIzhgg8HG2w9sbBbbIECyAQESIMkSWkZIs2rWnt73tapr36tyz5dvjd8fWYNaYma6Z9Rdv+LX+p6TJ19GROa7956b8SLifuOGkFLyPVw/KP9fC/D/d3zPwNcZ3zPwdcb3DHyd8T0DX2d8z8DXGd8z8HXGWzawEGJcCPHwtRTmde7zb4UQf7YRZHkr+J4HXwMIIbTXq7smBhZCfEQI8bwQ4jeFEAUhxCUhxGOX1T8jhPh/hBCvCCFKQogvCiGya3UPCCGmv+P3xoUQDwsh3g78EvD9QoiqEOLYm5QrI4T4shBiaU2uLwshutfqvk8Iceg72v+cEOILa9fmmj6TQogFIcTHhRCRy2UWQvyiEGIe+N+vJ8O19ODbgXNAC/CfgU8KIcRl9T8CfBToBHzgd6/0g1LKrwH/CfhLKWVcSrnrTcqk0FS+D+gFbOB/rNX9DTAghNh2WfsfAj61dv0bwDCwGxgCuoBfvaxtO5Bd++0ffyMl3tILGAceXrv+CHDxsrooIIH2tc/PAL9+Wf0I4AIq8AAw/Qa//W+BP7taWa7QbjdQuOzz7wP/ce16O1AATEAANWDTZW3vBC6tXT+wJr91pXu+bt/xFjD/6oWUsr7mvPHL6qcuu54AdJreft0ghIgCvwW8HcisFSeEEKqUMgD+BPi0EOJfAz8M/JWU0hFCtNJ0kkOX/QkFTYd4FUtSysaVZFjPh1zPZde9gAcs0/SU6KsVQggVyF/W9rtZ7vs5YAtwu5QyCdz36m0ApJQv0/TEe4EP863uYZlmd7JdSplee6WklJc7zFXJtZ4G/iEhxMiaV/074LNrXnQesIQQ7xRC6MC/pvk3fRULQL8Q4kqy6kII67KXBiRoGqq49lD9N6/xvT+l2S/7UsrnAaSUIfBHwG+teTNCiC4hxKNvVun1NPCngD+m2ZVYwE8DSClLwE8AnwBmaHr05aOKv157XxFCHH6D3/8qTWO++vq3wG8DEZoe+TLwtdeRawff8t5X8YvAReBlIUQZeILmv+FNQazHgrsQ4hmaD6pPXPebvUmsDb0Wgb1SygvX+ve/N9GAfw4cuB7GBa7pKOLvsDZB+B2aT90N57WvQggxTvOB997rdo9r3UWsjQLOA2+j2ZceAH5ASnn6mt7oHwiuRxdxG81Jx5iU0gU+A7znOtznHwSuRxfRxbdPKqZpTqO/DUKIH2dtiil042a9rRU0CYEgFmtQq1sID0Q0gIqKSPoEvgqeIJWsU6pHQILwBFrcIwgVQr/pL8IXYIZIV0HxIVRBrH1GgjszvSylzH+nTNcD18PA4jXK/l4/JKX8Q+APAdpHsjL/2Mfw7i7jOjqhL8hHPdyZGHpnjeG2Jc7sHyA9ssLyfBJtRSeSClCTLkHRAF3S1bOC8ZsZ7v+vL/H5S7vwX8kgVbAHHfR5A7/DJfOiQWmL5NLHfn7iOuj9mrgeXcQ03z5r6wZm3+gLJdeicUeVVMwmm66iGQH+WJzQDPF9lbJjIXrq1B0dpaoRnRVEWuoEjoqebfBbD3yaxWKcyY8G/OmJ23FcjfpmB397lZZ8BS/v8d/v+XOMiuT9+16+Diq/Pq6HgQ8Am4UQA0IIA/gQzZWr10XoKXiLEUovtlEoxbild5I77zvF8PAs0ajDxEQef8WiMZUgjAW88yPP06iaGLMGyvkYvz/5AOmETeCoyJKBUzO4eWiCW3qaPdUjN53il3/no1gfneOzT99xHVR+fVxzA0spfeCngL8FztBcQDn1Rt8xLY8/escn0G8uIMYjjH58K8+fH2Ly2V6C/RnUsgpJD7XNpu1Zlf9zcReKEeC2+AQGjB3sofJKnp2DM6RPKCQydQ4f34QiJMXTOZ6b3ESow/LXu661ulfEuszkrgSzp0e2/8pPE5nRcIZthCr5mV1P84W5XUyvpPFdldsHx3np/CAEAmGr5F9RWLzfw0o5iGMJAOxuH2tWI9hWwzB9guMptF1FtCfSpN87w/iFNt556zF+/5Y/PySlvGU9dNsYMzlVghHy2Htfprd9lXDF5A//+J1MLGYR52Lops/ox7dCo7laqNYUVnY3RxzqwQT2oEvk9mW0skqj00c9F6NWjNByPKB+KUm9QzJxuoOW3iJfu7DtCsJcW2wIAytaiLao88Wzu5jd38nIzkmC28vkvhbBj0q8mRg/8C8f5/abLtLeVSA+LQiyHvELOqoD0YsG9QMt3H7PGUTExxls8MDIOfp+/hzJUYXIosBcUVhZThDY12Xy+rpY37u9DrJmHalC4KgYjmB8NYs7EceNC7Q6OB0+//NLjxF0OGizJv7WAGvCJNSh3h3SvnUR29U59PURZJcHgeCZM8OIqkbqkVXKkym0vI06HqP7GZ/JddRtQ3jw6moCvSLo+LqG2F2ithDDKCr0fP8YgQloIaoLqQMW6ZuWURyBb0nqww5GQaFsW9SPZglMSba9RHxUI5pskBsoUD+ZQcm6yMkY1pYSyzv0ddVtQxjYSjXwY5K5Rz2MJ1NEJzXsbg/H18jsWKbrKxqJvSuUbmuwNJdCryjoFUH6gIkfkVTnmoEGEUBxNMv2956lXowQ0T28ZEjQUPHTPlIK6p3huuq2IQzsBhqpi2CNmxRv8nFyIbExndEj3YRfaOGWf3WI5aUE0VMWwwPzxGYkXkJSHg6RuqRzcBl/UwO3w0NpbXD0ia3ELhj81MBTqDkHFMnwpjkaDZ3clpV11W1D9MFBqBAYgsSExBkIkDEPfbON8s0sXgy+8uStJIaL2FmDS/t78G71EVaAdBVyL+ss0Ep+ZInF5SSKGuJHJdrOMv/uf/0gYXuI6gkan+tEuUelSGRdddsQHiyEpLjDp/hoHVHQiR2NUD2bob7Xpt4hMYoC52Sa0JT4yRAk5LJVcBWU/2uZsMWl+mQbsmxwV98lgkRA3HKoDXoojsAoCex/XsCPSH7wsWfXVbcNYeAwUIhNaIQzEWTGo94uic4K9HMRem+ewcmFmAWB8ASxSZXcKxrZSB0hBaoS0povY5YkWlnhhWd2ICI+hhqQPqqTPQnuSJ3S0RYi3RW++Pv3r6tuG8LASkPQ2GkTZH000yc0JPUuSaPTZ+aFbrJbV2jkJe/adwDfgpU7fM6f70TJOOxpmaH6bCvFB228VIC5IqCiM7OYptYjKWyHyOEooQ72bJziXc766raud3sD6Oeb67uDbcvISIBWF6gVFc2GpakMXt7jiwf3EEQkbV0FolMadw2M8bcv7MbeaWOcijK8ZZZ6R0h8TEWoEi8d4GV9aj0h0RmBSLtIR72yMNcQG8LAoQGNHhc8hXPnuhCewq63naVvzwz1rgD0EC3iYy5oZHctUTicx01Kjsx3kx1aRVEk7CkzW06iNgThPSV290wjfEG+qwiaRH1oBVkyYH1HaRvDwAiJGgmw5jTioxqKLdh/ahOXznWQ6i2ROG3w4zc9h9PhsziW45c/8NdYKwJeSFM90IJ2PorrajgNHa/dw55IcOLZzSg5B0MNEEmX6J+mGdg6h15Y34HThjCwUCXmyQiNTg8vAeZAhdu3jzKyfZLwqSzObVV+7/mHUGMeO3ZO8Ae/8n5q3SHtj03hDtlE9q4Qrpqo52IYMZcwGmBsLxE4KotH2tDGLebvEPTHV5G99rrqtiEMLCV4KcmurZPkbp/HczX2nx2k0IhQu81GCEnqtMZN3TOce26A2YdDtLY6ky92E9Z0/FBBLyp48RCvoaFWVezRJNqSgQghuiAIUgHPXNhMd76wrrptCAMLAeqmKueX8szOZPEKJkiYnckiJbiLUeS+AhOlDKk9y+gph8BTMXcWibXW6M8UcLtc1J46xoRJ8oLgV9/1WUJN8k/e83UaWVCqKsaFCOOT6xLr/DtsDAPXFba3z+FMxbEmDDqeVYikGyRzNUJb45/e/xQJy8F2DFpjVXxHoz1fojKbIAwFJ0/0oZoB6vE41rKgcFPAf7+4jzAa8nsv7SM0JAgIttUQjRtxFKHBqSeHyZwWiBAqHyrjTsRxjqfZOTzFJ7/8MItH24hZLoVGhPhxi9nJHDu3T2IvR/nQ3S8RFA2crTYj338GjJDlqTRa0qWvb4nO22cJUx5BoKA0Xivoff2wIQyMhIH7x1nZExDsrNKwDUINVFtw4kwvP/iOZ/HbXVYvZBnJLBBqIIyQ09MdRFrqfG1qGxghrS1lDj+xjc7uVbSyijIaYWK6hfJnO9k5OMPdm0YJ13n15YoGFkL8LyHEohDi5GVlWSHEN4QQF9beM2vlQgjxu0KIi0KI40KIvVclhRFy9kQPak3FLZqEixa5o4J6n4+WdPmzr91P7psGWk1wdKmLeldA/LiJcTqCN5agZpuoRQ378Ta67p5mdryFwVum0OoCpahR6YcVO8pMLY3VVX2rtnpLuBoP/mOaFPzL8S+BJ6WUm4En1z4DPAZsXnv9OM09EFeEVlYQgcAoC4y0w+abpljdJRGOgjIaQWqS5VsCEpfg7o4xWg4ryHuLdDw4jeIBF2JoNQWpwqUzHaBKRo90IxUIMx5+VDJ3Ic/UwS4y8frViHTNcEUDSym/Cax+R/F7aO5vYO39vZeV/6ls4mUgLYTouNI9AgtkxmPvY6cJJmNcPNxD5pRgZNcEQVQS6pLotMbKHT5fPLSHW37yCJbuM1dMsvO+C2y6ewJtZwlt3zIiECTyVZTuOvYmFyFAWgGZ/gKBJbE0/+qtcw3wVvvgNinlHMDae+ta+Wvx0l6TjCCE+HEhxEEhxMGgVkP6gmMLneR3LCICQWFEMvuZfoJIiFFU2PbYedBClLrK40d2UjiXxTI8Lnx+mKnH+6ktxAgfb0FpbVBZiGMYAb94x+NQ09CTLl6gYnZXmVrKvJY41w3Xusu/Kl4afDs3zRzskumjBsW94MynUH3w8z7FHSqR1jqpAZtDpwewZnQUD+p9IdaSQjEXR7m1jjoaQYQC1ZUoYxGUqIRLKX73yHvQd1TxHI3qchJzRUXfWbrGKr8x3qoHL7z61197X1wrf9O8NADhKBglSfqgQdDTILZnhdh5g4Gtc/yjTc1nq7GokbtrnuR4c7XGTUri6ToyEGgjZfAFxa0Sa0kgcy7i9iJ+TKJpAYrWDC012nzsmfgbiXLN8VY9+G+Afwz8+tr7Fy8r/ykhxGdoUlZLr3YlbwSpSVZ2SyLzgjs3XeLl8X6UtGT8ZCfzC93oVYhKmD3XSrJFAcMniChUp5MITyB7PNR8c8taOathRT2qK1H0EMJjKXRA1SWqI/Dj68tkupph2qeBl4Ata/tzf5SmYd8mhLhAk8n+62vNvwqM0dyd80c0dw9dEV3JAgO7Z6jvsnn50gBywUIq0DMyj+KB/ugy+mNLqA2BlwChhajtNlquQXROQRxO4ld11IsREBJvMoY5q5M+DzsfOUejx0NIcDMhgbm+Br6iB0spf+B1qh56jbYS+Mk3K8TsahZ9Nc3tA+OcWW7DjzpUV6OUGyb17pBWq8HEYhazKPCjktaWMksrCVKpOuXdgswTERotKk63R/yMSa0r5O5HTnC60MaB04OY8xrh5jqRwzGSEyHjb1bA7wIbYiYnDUljKcLB57ZSKsRoXEihRXwcTyMyr1CoR9DPR0lMhujbyyydbSGs6TReyZF6JgLvW+EnH/k6iWyN1GiAtaTw9IHtFCpREud0FE+gGz6NFsnc/Rusi1gPCEWCKvFjIdaoSRAN8W2N+lKMxk4b75s5AkOycG+IeDmFCATJMxp2v0txRFIoxfj43zxKvWYx81iAVCA22VzUqXWHOFts6mWruegTrK9uG8LA0lMgEOglBXdbnTv2nOfOraOgSoK6RsujM2RPS6L5GtVBnyAWcPcPHcaabIaAwoKBnwzpbCmS7ygRnZdUB3zeuekUqU0F9DGLO4fHMFcUtNb1XXDfEPzgaFuPvPUvvp/Jkx2E5lrQzAxJt1TRtYDS0RaiOwpkY3XG53PIkoFSVzAGKtjLUYY2zzHxSjehLrEGK9zXM8rBxR5UJWT+Uo74uEaoQmIqxM4rnPxvH1s3fvDGYPZEJZMLWdJDq1i6z+xcBmvCpGxECD0V2jz8SoQtLYssHuvGyUn0kkCWkyhbbKZW0ogQ4pMKdiPJE+4wXtGif9MCatpFL6sU73Jo3bcCng7/bf102xBdhHAFYs6icjLXnAqGAm1XEfNchA/uOQiBQBu3ODHfQSMvMQoCNyORKmzuXMSdjyJVqAyGxHetEMxFiU5qrNSipJ6JUL7PpiVXYXIhy+rRGzCiocZ8zFWBta3IzHQW1QoIQ4GzxeYLX76TjqdVzB1F6gsx+m+eRirg5zzcVMjsl/sY3jFNqEtCM+T9fcfIbl7FTUvCUCH9wRmU8Qiloy0IReLHb0B2pSxq2F0BylMZtFUduWiST9Qwz0VQG4L5O6Eyn+Ajdz9P6VPd+HGJtqKj1Zp01IavEyQCrBabP3rlXpbnk02DH0lx6WwHXotPchQ6cyWkcQMO00INEt1l7FaJnwiweiuMZOax+13YU0ZaAV39yzz+X+6j0icQm6v4GZ89D54jSATUPtOBWlaxnklgzhjocZfsCYEfkUhNosY9Kv2wuL/9xiSeSENSmU42t8aGAvdikifHhknnq8jjSaItdbZlFrA/UEKzIWJ6tHevcv5TW2h/ViV7ukbY6nL3Rw7hdHp4NYNah8DL++gFFfVSBNUWeIMNrPz6DtM2xCjCsjxkzCesGYg1uqk4HMcPwLinQK1ucmKlg0zUZiabhrMZUnvmWHmghm95LJQjCCH5ytGbUKoqUpcEEYmVbuAV46hrfL+7N41yYrFzXXXbEAZuNHQUPYQeGyEFue1F5g+143c7iIpF6gWLeizKwmYf0e4iHQU/VDAPxqlsdxjqXWSxEifbXmf++S68LXVcXScfbVArJNj86CiXCln2T/ajHV3f5coN0UVoFYE+ZuGXDSJHI7THykTnBbKmkX7OotYD933oEJnOEpR0ouM6D3WcozYQEE02mH2ih0o5gqaEBFtrBLZGLF+nciCP3FtmqpwiG6sTTEV52wdeWVfdNoSB/Sg4+QC9oGK3h6w0YlT6Q8xFjYEfvkB27yJPfuVmCgtJfvmRL3DXe4/xqSN3oFUUejJFxK0lImct/FBBOx1D1FV6MwXUm0r45xN0JctMnGsH4MnJ4XXVbUNMla1NXbL9538WY6UZXQ41iZ+QqPVmBEobruD7CooicediiBDUdhuvYBKb1AhMMAtQvrmBsmyguAI/FiLjAfqiTtjTwDoRIXH/AoVKlAvf929urK20QkhkJCB9yxJyewU3HzC4cwapwdCdE9jLUbQTcYLxOJF5hdzwCr6nggK9j47TdtBDeXgF6apoNUF0TmAuqySPGVhLAv1ChNpml+VjrU0u8TpiQxjY1HyErVI4kmd31wxa0mV8IYfiwfnDvUSmNPa+8zR+1qP1sEvjyTzKosHQ5jkuPdPPxHtBU0OSp5ubDO/54UO4m238+0rNiUirT3TUQKrQmI+tq24bwsANXyN1SuW+h49z6LktBL6CeiGKlwyRApxcyAsnNmOlHKb/sU9gQvuORSaXM2j1JnOydLgF7+4y3kCDrz27B5ZN/LNJWrYtoyQ93JREqwsGtl4xRHhNsSEMLKoqxV0eT+7fwfsefYnNXYvNxXEFZNZDGpL4qI6zGMUvGTz4vkMsl2O4KxbVrS6qLXBbfezlKMqMxfDNk4RmSO8d06wcz6OqIdpgFT8quTS9wRZ7hBA9QoinhRBnhBCnhBA/s1Z+zfhpoSVRbJV4b5m/fu52JlczmAWBUleInjG5a+85qkMeRr7Ow3tP8fUn9qIcS6DaCngKUgEl6hOZ0sjsXKbqmmhVlbqno/jgFU0S0QYihNgp80riXFNcjQf7wM9JKbcBdwA/KYQY4Rry0xQtJHNCUJ1JIiMhfblVGrvr6BWBk5O8PN6PsALC0TinC234XQ52j8+vveuvQZF07Z1jS/cCeh3qjkHJtth15wUWTrbiRyVokuLxFqJbioi7NxjDXUo5J6U8vHZdoZkmpotryE+TUpD40CzxMRWEZLacJPAVNBuUvhqhp6BNm8g+m4UTbRiWBxJ+9cC7Ea5CzTVY+tM+uL9ArRihNVFl7C82Y64qZIdXSZ7Uob/O9vw8Ddt4i6Z6a3hTfbAQoh/YA+znu+SnXc5NC5dsxsdbCe4qoS/rVJbimBEPu1Wi60Fzz1veJ1g2SYwLok/FUWsKqWQdvaAQhIJ6h8A9mgFfEEpBaViSuGeR5ck0lc0BjEdptSpEX9ygowghRBz4HPAvpJTlN2r6GmWvmTdNSnmLlPKWVLcATxD/YhJzWRAd09F1H7MgCI6nmns4yirJiyqBBf7bi0TmFeKmi1ESbM0tYhYk3mabyITO2MV2yDssrSQYHJ5HcQTm1hIq4WtLdx1xVQZeS5z8OeDPpZSfXyu+Zvy06kqU//DQ51i6WVIdcWnkQz66+SUCExp9DmHRwCgoVG61yT82zWBmlVpvwPRChl/+sU/z0tlNFHb7SAk9+yZR4h65TJWwqjP/ZDf/9NFvYH41xf956VZqnRtsorF2ksAngTNSysvDha/y0+Dv89N+ZG00cQdXwU+TKvzKEx9AqwuEFkKrw//42tvxh2z6ulbAF1i3raBNWFyazhPXHbS6Qv4bJv/lv34INeKDJpGh4Py5TpRZi9UzOd5320Fa983wJ3/2KKv3OEgjpGvv+o6Dr2a58m6aCeRPCCGOrpX9Ek0+2l+tcdUmge9bq/sq8A6a/LQ68H9f6QbSaE6VfV+gzpkkR1ZItBYo/U0nk1sNMoOrlM5nEQYMdC9xYrEDLx1Q69So9fkIXyFzUKe8SWIUFNRbitRrJo9fGkF/LomXl8iahoj5TIy1Xkmca4qr4aY9z+v3XNeEnyZcgVLSkLpE9jRovNjC6oCHmQfFEby/7xifGHsQ3Yb5J7txciHRgQrWwSS1TZKP7n6RTzr3IhoKdq+HfjZF2O7iawrxR5aICImhBkzNZtHKN+A2LhENSA+tsvumMcyTUXIPzKEnHcItVbSa4JvLQyiuwG3z8W6qEeZd/LNJSptBjXv87ycegEAgowFaQUPZVEW3fLyCxXBmiYWFFNMXWsm2VGjZuXhFea4lNoSBFSFZnUkT1x2c7TarT3UQzEfxlyM4rQHnxjsI8y67hicRo1GsixaBKYlsKaKqIWpnc2OLvqQje20SUQf1bIxMV4lLv7UFQoHUQwrjGeans+ur27re7XUQ1DSEFfD84W2EBYN6Z0h0VkG4AowQPeqiTxucnmsjuXuFYEcVxRXc3jFJMBVF15upbpXBKkHRYGUiQ2BKHuy6wOxjAaoVQCBQXAE34nKlNCVSQvaIAgkPaQXUegJodbh58zjD7UvoVUE86uB9vYUgUPA7XV759C4UrxmFFlMREo/H0csqO3ZMwGCNp6aHGepbIPWsRWROQ+mqo5RuwHQGCEm2pULlbTVu2TSBlW0gzZCe1gInZjs5NdpFeEuZwnyS3LuniRyNIrQQLwa/+YE/IexqkNu5hOpJAktyZv8AEcsjlIK92SmKWyX9+8bxChbvvX99Y3IbImRk9vXIHQ//LKv7GlhRF/dCEmWgRuZLURYe9FHMgHy2wurRPKEpod1BPx+h0eajpjz62lYYn8uBgEjURVcD3P1Z7LYQtSHQ6gJ3yEabtDC3Fzn1nn9/Y4WMohEHqUA6XaNhG/jJAHkpxuLbXCKpBmLOImE6mNuaW7DuGLhEOFIFpZnvcuJYJ6GnEtY09nZMUZxLYm9pkBlc5eZ7zqG4kHwpQmwaqtPJddVtQxi47uqs3OlRqUYQcxaYIX48xJgw6UiXye9YJJQC51yKW+44zyvPbiOTqKMXVbL5Mn27ZkmcNFBiPqc/uR1rTmOkb46abXJyoYPcaZ/S1gD3bWWkeQOS/9KWjWio6CdiKD01rAmDWFcFLykZu9hO+OlWvEAlsCSvHN6MOlRl5XieoLtBqRxj7uluqpsCwoqO+YEFGl0ep0/1oh5N4DR0On/hIlqrjXsuiTDXdw/BhjBwyWmmO9Tq4JVM3GyIbRuo7XXUisriPT5zR9uxeiqItEs6XsfP+GzvmSMMmjkmCEGJeyyuJjHnNfQWm8aIjVBC6r4BYzH8LgdjfONFNK47pGyyK60VSbKtSnxCIRp1yCbr0GUjHAU/EfC2/nPIULAw1oKa8Dj9ygC5bBXltiLmiop5PoJheug7S5imh3Ehgr8SoexYBL0NpKfgr2/qyo1hYAJBeTnG8iMNynMJKkMB9Qtp/M/lCX0FraWBsaLy+MURUq9Y/N6jf0y4arLjtjGWF5LUZhI4LQHJOxex5+M4DR3nZJogIhEpl4mxVjZ3LtL6rI65eiNmPBFgzOts7loEK0CpK9x1zylW7nGxzlnNbqMlIAgUynfa/IvDH0RGA04cHiByycCaVzFWFVZO5rFmVbyCyaa7J9DLApZNjCWVSy/1srTPpd7vratqG8LAiUiD+K4Vio0IQkDY5nBquR1j1sDu8VFiHtaCxu6eaXTD5+GB89w6fIlIXwVni40flThtPtHhIk5LyM7tk0Q1F29njf4dsyiBoPv2GeLpOsJfX5U3BH3VCTQG0qtc/Kth5A4PtaJSII7s8NAjHvlMhcVZi7pvcG/fGIeWuynub8PJBqi2gpcJEGZI/XyayFCZk+OdyJqGkvSYe7ab2Jxk5sUuEntXcFpuQAK2X9Y5PNqH2NTMfG2tSsLhGoXZFEE5wtyyhehqMPp8H1M7StSmE9zxtjO8dHwz8b4CXqDink2i1QS1YgQCQWRWQ2mvYdxaZ2UpAZ6CfzaLbL8Bs6+GliRxxERmXPyYpLBN0p6okO4okzklUGsKoatilAW1ySRqTeHo17ahlVQ0NaReMUnvWqax2UE1A2KjOgiQB/xm7xwAACAASURBVFNUT2YRNRVrTkOqIFduwGGaooXUekLaHzcIjGYI6dyRXoqLzRNe4lPNqHN12GPrTZMEEYmTaUaIC2eztObLLE1kQEgUNcDdVUPsLuHtrIECiivYvG8MxRFEuivrqtuG6CJkQ4UOh4V3wc39kxx5cZhQb570snqvg7RVOvtW8AKVM6OdJCYUqoMBgdrc3bksWlCBgb+U2PkIMw9LfEsjka6z5Z5pxktZTpzpRXQ6NCYS66rb1USVLSHEK0KIY2vctF9bKx8QQuxf46b95drJWwghzLXPF9fq+690D6k0OcJCkRzevxlrSRDtL6MlXcyLFlrCY2Elhf10nmjGxrm9irmoYpQE1Z0OiiuITStM/kTAzCMhWtIllamxq22G+VoS5S9yoEo2dS2RHFvfP+3V3M0B9kkpd9E8G/7ta+H43wB+a42bVgB+dK39j9I8P36I5rHnv3FVglyKEFR0kHDb+48THExjHY3S6PZQL0TRz0eod4aoaggXYoSmJHNWolkeDNRo5CTKmTjxizqm5VGrmxTdKHNH21ndLujqWWH6mz3U2zdYRGONY/Zqujx97SWBfcBn18q/k5v2Kmfts8BD4rID4l9biuZCeby1RmhJnj64ney98wQmDG2ax482662+CsaX04Q6/MWHf4f5+0I6s2U8W0duquNkQrRac1wdizqMFzLI7gbG1jI/O/gEb/tHB0juXt9zNK6W2aOucSIWgW8Ao0Bx7ew4+Hb+2d9x09bqS0DuNX7zW3nTqjUUR1BdjSKjAX1b5pmZzZI7GXBxspXEcAEpoDGZoDwEIoQfePnHEFGf6ePtPLbjFPFvRmndvExlMKTwShvlsTSOo6NMWbhnk/z8kx/iyy/tRYgN5sEAUspASrmbJg3qNuC1zgx7VfI3zU1Tk1He/shBurpWae8oMHOwk+x+HeunZsFRKU2kkD0NQrMZodCHy/iLEYxLFkEqYLaeorgjbEY8LImXCon0VeBiDPrq+IkQNInZXmdpJn21trkmeFM9vpSyCDxDkyecFkK8Ogq5nH/2d9y0tfoUfz8147dBrSl86fguZuYyzF/KIUJwk4IPdR4gOqERmVNRxyz+w0Ofo9Huk4vX2bF7HHVHCT3ucvrFQYj5/NL7Pke6p4jUJHbdQA7WCeYi9G5ZQIn4OLMxItkNlmJcCJEXQqTXriPAwzQ5wk8DH1hr9p3ctFc5ax8AnpJXCPyJpE/qiEFXRwHFVQgHbOqdIX8wdi/2FgfNBi8b8qsH3g2qxAsVLj4xiPJyCvNIDC/vETtp8vGx+yhOpdGqCmFV56buGaQhKTdMwoqOyDrYlY030egAnhZCHKd5IOo3pJRfBn4R+JgQ4iLNPvaTa+0/CeTWyj/Gt5jvrwvf0ajeYVN3dcJoQCJuI9oaVF/ME0k0qN3a9Dpt3EKpqfiBit3j4WQk1a0uCHj7h19iKL2MSLoIH/oGFzl0ahC9oFA/kgNFcuvgBPrC+hKwr4abdpwm6fo7y8do9sffWd7gW0TAq4LiQugLfnnr4/xC+f0UCzGko7L7HWc49NRWYquCyi4HddZEHymzPJ+kvWeVeT2NPmfAYI3PPXc7MuYTy9o4g5K5Ax0oUUnm5iUWJrPoBY2eSIFj9RtxPRigovOfzr0ddcpCKJJ0W4XDT2zDS0pq3SFUNRDgnk2ilDUWl5PgK3h5H69oEVlQyD+nk47aaGej+FFJ+0uwuNSMIj+47yiff/Z2Wo/8w0hve00h4wEi7eI830J8+yphTSeQgtjeZSLdlWZK8bTLTY+dJXu6ub1LnbbAF8Rbaigxj0Y+ZOlej6UDbdz3riOIEObukUQTDsIXPDs+hOIJph/aeDO56w4ZKBgXI9gdIUnLQVgBScuhdC6L52rIpAfLJvtPDLH6rjr5A5DauQKqpH4pSVjTeeDOkyhVDS8bcqbQThAJmyH6/SkGts7h1Az6b55Gbdtgo4j1gHAEys4SmROCyTPtaKaP/6lWcscE2VSt2caHoU97eGWT5Xc4NJ5p4c7tFwmTPluGZxir5IjMK2wdmaLwZAeKq/DInpPUhjzG53NkclW2peZ5x/AbnqF9zbEhDCwNiT2dIPUDM2h1gVfTMX5kgU3/7CwLMxn0iEeQCCgMWURzdUJfoNxT4KUzm0DAUi3G+ESe+rYGl57pp7rFJTKv8ML0IMJWMM9G2NU6y5cO7uGLh/7e8/q6YkMYGODD979AR7SM1+5CIKh8qYOTS+2oUR/lfAwj7VB/rII9E8e6aFGZStLWWSR1zMB2DKxJg3SmhtMSgKNg3LUCr6SQsYBQh2cPjqAXVaK5DZYkfz0gtJC5RopDMz2Iso6wmuybRkNnqGORYMhGOxonOJvAKKhYt66AAisn85RucmnMxXDaAhqvNAmAesahOJXGyUgSpw2cdg+1qiB7bRr1DbwR8XpBOirPvLyDRtVA8SD7TZPiTo941KH4yR5My8Nakc3QvSXJRG2iUyrROYEe81ByDmpVwRupc9POcXLpKlJIgkhIZcRFRAKEBPN4lPTz1rrqtiEiGnrEQwrAUWnbschsvAV0SRAqNHIKEcOjNAjmgkZoSKZX0vhtIVIBdSKKAtz7wAkmaxkuFbK4nsbg8HyznaeizFgkR8FNwurNN+A42A9U1IagpbtI8dn2Js1fSDqSZcJ9BcpHcxjDZdQG5I5LtrQv0rFtsbnpZVOVwdsmef6JnYzO5OlJF9nSusj8k924dZ3etlVCU1J+uIa5bxkr01hX3TaEB1u6x9v2HeHwUjdSg2iujn86ydzRPpQQ1DR4noq3xaG+WTA/2o0Rc1EsH8vwmPlaH96Iw9aeeVY/3sfKToHXGZA+YNK/bZXZWidhqFB/oaUZLlhHbAgPtmsmX392N85XWonctkzkq0n8/ga1bokfa6ZedMsmkfMmqhkg6irqyTjtXzKxHZ3qFpdcS4VzR3op9ymY24skLqqUtgc8c3QbXrx5voa9rUEwsvHOMrrusKIukU1l/BisTqcpDwFLJskxaNxUJ9QlalHD3VFHHbdQHEFjc4OF26AvVwBPoXgqh1FU8KMQNTzKIx7WrNockaRdQg2ipyy2dtyA++QaDZ2GbZDaN4+WckldALXDprArwDgdxe92+LG3P0FXSxE3G5AYU1C1kPz2Jea/3Nsc384Lorcu8/A7DlF6ubV50JQP5phJS0uFICLxkpKTE+ubWnFDGBggXLSYmcwh5yxKQxB7IUayvUJgSvo6VvjEVx5m5YlOfuLeJ/GjoJ2NUn2qDefOCoRQ3uITMzye/vzNOJsafPi+F6n3BriZkNoLebInBAN3TtL7mRtwK63iCqyeCtasTmKo2GTupOGR3rMYZcHsaoqwu0FtxOFPzt9BbbtDo9OnusVFno+j+KCnGyy90IHdGaAsGXx1coTYJRWj1MzpU9wmuXC0h+kfvAGHaaEGt3VNsv2h8xRXYyQuKei3FPjsK7cC4LsqYVUHIfF9BXPcRKkroMDme8aJLAruHxjF3WwjowGdz4UET+dwWiR6Gdx0iNFXJXVeELg34HIleoih+Byf6SR9wMRNQa1ugiKp9QZET0RAC5E1jWSs0Uw0N6fwm/f8FYu1OKoteeLkNuSqiaipRH56lvqtdcJeGz8KA3tm4FiS4h0u6vIGnSqvcSOOCCG+vPb5mlGnFFvhXLENr2TiPVzC7vYY6ZpHjfkgQW2Atqpz756zdMbLoEoae+r82h/8EEuLSQoPNIiMGahOM6Xi1NO96IaPnG9Oi0dPd2J3+ETPmht6C8HP0Iwmv4prRp2SCswe6qBvYAm7bmBmGhQbEXTDZ9P2WUq7XURPndFSC6Nf3tQcIYxFEPcWyL1oED8YwU9I/JxH2z2zeHHZPOz6vMAdqaO32qBL9DtXydw//yZU/u5xtcyebuCdwCfWPguuJXVKQs+tM0xcyqMbPomow+LLHTg1g6kXuml9Rser68zNZ5AqaK02kQVBeT7ByEdPURlspmAUtsrERJ4gGmLPxSluD1HHIrhlE9FQ2N02Q91d36nc1XrwbwO/wLdSzOe4htQp36kydrGdD952AG82xspyAifvY42azYOePryAHvUQRZ1ar4+4GKPaHyI8wflCK0ZRQW0IpBGCbJ5jJNzm0T1+XNL1twrvvusQh+e7sZ0N1gcLId4FLEopD11e/BpN3zp1Khanb3CRL3z1ToxVhVSmxj+792nsTh+732VmNovX0EgOFIlOaaT2LNO2fZFIZ5XwL/OIkQpibQPnI7tPwt1F1A4bLxlCzmHmYcmXnruF8mIcRdl4W2nvBt4thBgHPkOza/htriF1Ci1k/uUO8jcvEFmSaGrIJ77yMP1DC2grzeGZPmewObeEdnuBpek0luYTHkuxsksSXIgTvXUZI+Xw9FO7cU+lCGUzAYd5rjkCCWNB8/Cpk6k3a6PvCldDX/1XUspuKWU/8CGaVKgf5BpSpxRF4iUks/MZCnt8iuUoN997jqHkMnpFoJoBXt7n0KVeahdTaAmP2ee7SV8MyQ6vom8pUxjLErVc+m6dxkuHBFWdyIKCu83GmNcxFjTSLVXaX/6Hs0/umlGnZEMlMq80h1FzOuGCxf6zgzx5bgvq3iLMWihVlbavmBj9VayjUaSA+QcDdudn8P1vTX9X/6qbTdtmIRRsefd5gpKOHKoRDtoIIZn68PrO5N7UerCU8hma7MprSp0iBLstJNJfQT2QAiGIXDTw4lDbDlZRofW+WabyGZTRBN7uOnLOYtfwJE+dHyabqeH3u02a680+xeNdRJcURk8MI251MIwAezzKv3/3Z/jYoQ++KdG+W2yImVws1SCMhtgTCextDSKLgtb3TeKmJJR07F6PQj2COmkhNUn0QJSWbcss1ONYUZfC2SzVsxkeuv0kZrpB/85Z6r0+j/yTF1FWdOyagbWk8AvH3r8xCdjXGw1fQ0R9tuyexLxoYbdKLky14bV6PHjrKQgF4QsZvHRAkPWo9IcsTGbpipe4o2sC2e6g1QTPjg3h2jrjcznanlf4wlfuJIiG7OmfInr3Mk7DwFle3+32GyJk5NsalHSmEmmcQYeB7iUWK81d86cLbQB4cYhf0qiOBMhoc0x25MAQ5oqCbA1p9LkIV4WSTvKsihuXuB0ekVSDYy9uxm91yeSqaOmQyXXUbUN4sGIFRKdVarMJfu62rzcDlqdTRPbHmL+UY+e2SfzoGhdYCoYG5/nFu79KmPbxd1aJ95UQeog6b5I6p3LbR47gZASKGWCvRvAzPnrEo1o3KR5vWV/d1vVurwNZV7F32ty15xy/+dKjhHsrZPYsUR72UesKJ843s+VuG5zFjDuYqs9/fuXttD6ts7dnmspKjGjcwc97lLYEPHNpMyIE61SE1Amd+HkdGSpEX4yjD79R6uNrjw1hYCIh5tkIB6d7aGkr48zGKNctfm3f50mfESgRH6lJRl/s42M7n+TsTDuiqFMYgVcu9pM4bSCEJLtf555bz+AtRdj73pN4SYn6yDLingLW4ShyXwHvwg2YdSqiewSmRNPCZmhdSCKmy3889g4KDzbQLlnIrEtgSX7j0KOEKwa5I4KHHz6CPmniR6E2mWT1Fp8XXxhBRgP2P7Wd7K4lyqdyVGeS1AYCDC0gt+sGDHraFZP4FLjnkngpyY5dE6xOpdnVNYNcNbB2FpvHobU7hK6K1CSlYdg/30v0pgKBIdGrAqWmsmnPNDgKgSkpHMkj+msQ9xApl9LJHPMTf2/d6bpiQxhYj3uoDkQWBV7W5+LXB9HKKkefG0bGAxQhUdWQRNyms3OVRGeFcMCmeDGL+FqGICLxoxLVFpwf60AEgthQiVAHORFDrBqkUzW8Fg8t6a6rbhvCwCxoFLdBeYdLuq2C3e9hbC4TGynQ07WCcyCLHItRnEkyf6aVxuk0gafwI/u+SeEWj+hQiTDtoQ5VsaZ09KJC/Vwav8VDqwmQ8NFNL6FUNXLpG5B44uUk2lCF+DmDUiGGMALsmklxLslyJYZ12wpezkcvqSiOwE82lxz/7PH7SbdUcV0NbcnA0H38eNObjaEyqaMG/tY6Zm+VPx67kzDhY2/QBffrCkWRmLpP7IFFKGvo0ybqrAlmgH8xQXEyjbGkoVUFiQmQeohmBGTOAN/I4pRNQl3iHcgQnxCoHTbKyynKwwGm5eFfSGBqPq3P6JRX1vccjQ0xkzNVn1IxivQU9KpCqIOf9cBTCHttpK3hqhLRFeDqIZqQBAsRCtshe0KSaa3gnc1R6wlptMLbh87xtLYZWTGpFSNEy4L5k60E9/mIxg1IPHEDFemo4CpERoqI3hrGnI4S8RGTEbKtzUjySP8sbbkSxoko+eHmWrFRCymNZkiPBYisA2mXg4s9NEomQgtRV3TqPT4iEBiLGpHpG9DAoRTExnTMZRX5XKZ5tGS7j1AlRklQOZXDnNO58EI/S4fbaDnh88Hew7jb6jg/ugotDtPvCkBAImVTez6PmXSa5x9pEnNJQ7MFXjqk0bbxQkbXHaGrUu8JcDp8rGXJvX1jaDEPdTSC4tEcXtVBbK6S2LmC9bOz/M8nHiFcsigdaSFsaBgxl9DWqFYsGtttpBTk9ixCi4MfC5s5gHxB19M3oIEREhIe5pxG8bEa3xzfhHohirK1ipCgRn0QEAYKpXKMEEF+eBmZ8IlPghLx8WejICF+OEJY1VHVkNKLbYQNDdHZwO9roDYEM9/3DydkdO0gBRQMFE/AWAzP1jF2FQjPxvHvLBMUDeojDSIRl3DFYLkaoyVaQzECSvc3YNkkjAUkT+lUBgPUqkJLooa7rU4kY6OoIdqE1ZzMnFzf9eANYWDhg14VeHFJchTS2SqdyTJBVOJMxcn3NQ/Zq15KQdqjfjLDYi1O6Kq8Z+sxtKog3VahvMVHy9tktq5S+ZsOwiULu2jhz0QZuHOSvQ+dxf5/2zvTILmu8zw/5259b+/d0z37DGYAzGCwLyQIgOAigCZNSrLlRLYsWl4SpeKkYjt2OUmVU/GfVCWpSuI/TlJ2xbIdl1OSZVkSrcUWF4siBYkbIOzbDDAAZt+n9+67n/y4kKNIskyIwLAj8K3qmp5bd6r7fHP69Dnf937v292GS4QQ4pYQ4oIQ4qwQ4tTta3fNclLEA0IVELC+N0QIyfjVvqiSkfVYWU6jLBuIADJvmqjbatTfKHJ8x1Ve/uPDBMM2zqk8W7ct0J2rsTqTpXq4BVmX2JyB6HKYq2Q4/dUxusfaN9lzTEq57zvU+++a5WRoq4gQ9IrgkYNXqF3sID2uEVsXpHJNFC1EqhDkfJqP1WlVTFQbLqz1MPLsOPlsHdWBDrPB6oke+l8SFF6MiH/usE3+RZP6UhKny2e91qaGfd8Hd81yEk3i5QOcsRZX1roZ+mIz4pup0LwWiRiJToe9W2d4eNNN9LiHH4eVuSynTm+lUrdo9IVYqkdr0GPufbByOERdiKEZPiuPehAL0CoadnljGxHfboAl8KIQ4ltCiF++fe2uWU4G1SbCF/R/Rmd1Kc1j//NNklMKXloSFl3iFywKuRq+VDhxYhdiMk7m0DLxjiZju2dw10yUgsNCM43SUMkNlxjbMUNu9yp9HRXMWQMcFT/voVjtyXA/KqU8QPTx/xUhxGM/4N475qZpOYv8OYW5j0bH409+/ji1MY/dh68jPYXhD9zgmb7LOIGGtRRRoioNC3cyTT7WxFzS0K9ZmKrPM4+coTyZZ+6LQ9RfK1L+yz6cjoCnDlzAmjIIWxubHXhbryalnL/9c1kI8RwR4WRJCNEjpVx4p5aTCElppySRsHGvWxj7SgSXs4x/ZQS2uFy43s/sqWHcJ6u4HZIgJomdS5E8uM5br27H32wjHZVz1we4sriZsCPAO1qntWZhd6qYPQ2+NjmKN+BR6Km0V1VZCJEQQqS+/Rx4CrjI3bScdFQoODiXsjj9LtVSHCGhudlj2+YFzCmD+uMNbFuPrHMagnBPDft0Hi8ToM/GEC2VeLZF8YEl4tMa1ksp4rd0tuyao7UaR87ESRSalC+2X0WjC/iGEOIc8BbwV1LK54ksJ58UQlwDnrz9O0SWkzeILCc/AfyLv/cVBMiSgdsRkD5voMUC3G4PhGRysUjm8DKGEfDUyFVCXZLev4ZTi+EnJGpLITELsXWF1mKSlVNdZB9bpLQnRGpwY76AXlbJ7VylOZ/Ez7cZN+02B23v97m+xl2ynEQLsXrrNFfj2EVJ4ClkC3XiMZf56Q4alkFjNc7ztR3oIw0s3UO0VPysj7auUR+EYKBFMmnjFHSG0+ss17oItegfp7YEq5N5YmsqHUdW2muJ2BC4CgO5MoQCoyLI5ho0ruRYutAFmqQ7XaPYV8ZMuHgtHdvX0Ist+gbXMNdERL5eidGomeztm+ONm8MceuwKYkcN1VYIdYnWUAhMyfLprg0dWlsEWIv7LP/FIENbl2hsCiiXEgT9NolZgVLWuHGuj/R/TeH7CtJVkFLg2RoFq0Hi2DJ6TSA6HWTZ4MKL2/i3B77CQjONrvvom2v4CYmfDPHyPsHgxsoZtEWAfUel+ojNrRudCE+QOm1iXLOoHbDJXxQYVYWV/SYdmQbZrhqW7iFDwcd7TxCECq3dLcI1A6lLuh6Z5z+++QEWXu2n2YzRWrOIrSkoHQ65nipiqf20K+85RCBQb5ggBcIX/Ltf+SQdRxahbFAdBnMF3BSsn+rE9VVmJzox5g1+/cTPkfj9DD2FCkZJ5ZkHzjN1s4h5LYY9YsOCiZ52afX7/IOxc1TrFmr/xooitYUjYnK0WxZ++zeQtkrX4DrrlQQd2TrhpzpZ3wVisImcjiPCKPNmrgnsI3WCmThSiZzFo5MfKLsrhKFCPtVgaS2DbkSytigSJJhLKuP/4TfvL0dEP4jqZNasxmo5iXkyweJcjuVHfBQXvKaOMtSIku5bWuz8mSv483GCTABFB2kG1MY8wu117KkUrq2xuJqBeZPgWpKnHz5L6oaKTPmoG5tvb48A4ygRE32zSybVZMeHr6InPISj4PZ56Es63pJFqEL6FYvr5QL5kXXi+SZ6zMfIOnxg/3nCmwlElwMiYlZKTaJ4gq+c30Xm6QW6uirURzeW2dMWZftvNxBu6l9l9lwPk1c68B4M0BsKvS+ErHy8iqmGhKHCes7CeqOI3RWRsM1FFXNVcq1QxO/0sK5Y2D0BRg3sLnDzAQP9a8xMFtHqKuJ+tNoxdZ9Qh6npAqoN1s8uggJ+j8vMsz4f2nwBTuRoVEz0ZR27OyA1qSKyLulbIZkPz7NYS2FNGiQOr5Lpr9Dsirhq34Y1p/Hjx06z6VP3YdneDVQOPzge6QHnQmbm8xzfdxnZVNH0gE+fO4iXBCGg/2UXaQVoTYmiSJysQvm5PrYXl+g5Nsv6tTyV6Qy5cUly5zpasUX9uW68lOTFrx5g/Z/fh9w0gJnfGUVYPmpDQanoxNSI3e4txtEWDDqOLKKZHjc/KiJtyyFQr8XhyXWqD7dIqC6rX+6n46xgePsCy4ciwyn9XJLqozaqC4PPO9RvbWynZ1uswWGgMP+oACkI+2z2Ds5G101J9pKCasOC2RkdieMhhcEyTlHlUM80X7s+SvKkxcuN7SQeq0DMxfE14n11xs8MMvLkFKPpZf7m6kHq/Qax1fvQsC9hOsRKCnsO3eTkm6PMvrSVi89E27LSPp9UV51wMYViK2CENN4o4ORDvjq7C6lIjv78aS6u97B4uptWf+Qu3ppLgiG58eYgk5uKGD4sH5FYcxs7trZYIhp1E/ZWOffSGGHGpz4Qzeb0uEq2u8b7+q+T6GwgPAGugpsLMVcUSEapx5f/+gDBH3Xh5QJSJ60ouAoQCLxOD0UNST+2xJaxecSDlQ0dW1sEGD0kYbr4piR93qC1xQEhqexxKc+nOfVfHyAMBcZwjXRv5Afn7WkwOriICAXe5hbzx0JQJb4FHZtLICHW2yBbqJNLNSm/3sVSLUlj8f+fqvJdQ0z3WV3IIHXJ7/36/4CWino5CZ5CclLD/8U1Yq+kadVi1GbThEUXrxxj4lI/alNhsHudTVuW0Vc1VAdKlQTZTWUe6JtB/XKOpZkciQWJdzbHLx795oaOrS0CbKo+Sk1F6Wvyy5/4VeIzGk5XgF5Sef/HXsPUfMRTa1iTMayeOvGkQ2wlaoYZOTiF7WvMnushMxFJ2IZlg9GOFd6aHmTtoI85r1PvF7Czxl985vENHVtbBLgV6CQ2VxACNBvUQyWSNyIPz9f//UPMzHVguzpOPmS4Y53mfJLAhK5N64zPdbG8msYYriEVcEdapPqqvHl1M/5SHGNFw+71OPBjV7DXLHY9M76hY2uLAPuBQn02TXgrQXW3yzObrjD6UxPYe5qM/NZlFCMgDAX9L4dcutmLVCWde5aonejkp3eeIX7BousPLco7JGI5hvx6DrWsEVtXMEoCPePgSwXhKJyd6d/Qsb2tbdpts6g/BHYRcRw+DowDfw4MAbeAj0gpS7cVpn4XeD/QBP6RlPL0D3yBmoqxplI4tIjjazw3vgc5nSAxL3h1YRdhIsQJdZYf1MCTdG1aZ+2tLtz+gM9d2YcoSG79jMSYF3gDLtld67TOduMUA7xtDuGaxcnSFvpOSEqj7fkl97vA81LKMaIC6BXuIjeNVEA42iAdswm/1IFXN9AaIuKjpQOsGY2dO2bwRlrkeyp4f1mM5MXrCh/f9TpBInKoTe9fY8vAMmv1OMePnSXVX4UFE2Mt2j8vPCII92+sK+3b4UWkgce4LRkjpXRv+8rdNW6atFX8NZOKY/KxX3sBPAW728c7XGPblnnMQ2tMlXIYly0q43l6fu4Wqi3wCx6feOtRtIqCNWlQPdvB7KsDNJcTfPXVfdiOjlFW0HdX2DSwilFWUM60mSstsBlYAf7XbWnFP7xNQLlr3DS/VScxrbJwvcgfjR9BBIL8GZXE80nGb/YQhAqN+RRCQmhI5j4zjF0MyRbqqBUNv/r7kwAAEmRJREFUr+AT7q3hDTo4xQAR91EHG2hnk9iFkPcPXab6XA9iV5X0o0vvKGB3ircTYA04APy+lHI/0OAHCx3dMTctljchBCXvoKkhWodNeUySnPeJXzewHR18gb2jRW64RHmXj5BQXkhjLiukxnW82QSUdaQmsZIO/nwcNyuRaY/PvnKY+mNNin8Sb8te5VlgVkr55u3fP0sU8KVvf/TfKTctlIL6mEvQ0qguJ/FsDaOqsP7P6jQHfNSLycj4aSVGpZpAq6kotkB4glZXiG9CrKSQvaKgVSM79vhQlSAZ8r7tE6i2YLRnmekPBwxtabMZLKVcBGaEENtuX3oCuMzd5KZJQV//OsaijjmnI6o6drdP8FaOnTtmkCqMjswTpn2suENyRqBurqPVFJIzCl5Kwt4qWgv8rE885lJfTFIYWue1qWH6Hpxn4o0hzKTL0ivfs1rdU7zdXcSvAZ+8bTu5D/hP3EVumnQVml/oIhxuESsBQiI8gRRw+VYv5mp0n1rWaFRNdn/sInIiSTBoox1fxc8GuI7O6vGoHleMN/jXj3+FxjeKcD3B9KUevGyAa0dH6Y3E26WvngW+X5n77nDTVInWhN39c5zZPoxiR7Lgig/SF3jHK0xM9JKeVmDW5ERjDF2F7AmTykgM3RX4cR/KOs8++jrPPfcI1/cUGH16knPXBujrX2fhSidH9tzkm+7WO3pr7xRtkQ9GRja/JScOqiR3SeBkBcbjq6iBSsp0COcyBCY0BwLQJV7Ro9QXYo6bOIUQ85qJXoOXt45iPrBOeTXJtJFFW9epn+vGTMJEqUjHa/qGkv/aIsDCF1RHAtxKCtFSWTvikepoYL9WIFaWzO8MYdRmuHeVG7c6SRUa1OfSoIe0Bny0qsrgsSm8UOXmfAFFiw4eh3umeOFGjlgFapslbiVBoXUfCtNpCQ9phrTWLaz+GsUTOrVSnOawR6MXpCbZMbjAzEoOYYTYl7PEllUSl0yy5yMDqYkLA0yd6UOsGihTFl3dZb5ycg/WgsLy4x6HHhon8BSaH9nYhHtbzGBFSPI9EeUplILV/ZLYdAxnwCU0ID6l4Y8q6OcThIUQbbRGwnQpjecxHqoT47aCq6fi1g3UuMfSQha9rLLvQ5eZrWc5M9+PtFUca2OH3BYz2C8Z2K6O+2ae6mIKhCR7cJn9W6eQAy2sFUkgFZy8JOzwsBsGpat5pIgElVorcbxLaQYLUSXjJ0YvQCjQWoIzX94R+YS+mcJc0An8+5AXEergX0nTHPJAkaQGq6yWUpw9txl10qK6GfxQIT5S5uHRSWKTZiR4pMLBkVsIK8AtBNw62U+us8Zry8P0DqzRGnKxO0PIeLgZiT3o4pfbTGJ8IyBV8HIhsQWdTUMrOK4W6f7qksyBVfS6IAgVdDWgEKvjFIJIsz3lc3JimN7uEiLhIwds4oaH7eosradBkYTxAHXZ4OATV+jtXSc+cx8uEaoNWk0hVoaZS90MFdYhFpI9r1E9VcQuRGwf29V56dY2OjaX8Ds8UhcNUpcM5mai/ELPZw0WLnfifzNPUDUwp2LEb+qYIxVeOzmG7Wk0N9+HcgYihMCQ1AdCVFswfrUPfUVny7MThNvqhPGoQ761kCRlOQghUeM+vgXve/YkA4OrUNNZ2R/Z7yhelCtWW9DqDnmwZwbR4aAI2rbT857CT0m0hsAaquHHI2t1rSa4+uVRDCMACdqKgdRDGi93snatg8DWaA27hFKh3DLJXFFBCg4dHOfnPv4SqiOw9zbJDJU5OT+IfsMkbzURC+3Zq3xv4Qu8QYek6SATPmpN/dukZ9J0yPeXeeDoOAQCjpYZeCFyRTyyfZKvfukBTN2nujUke3CZt97YxideeAK30yeoGlSmM+zrnqPnyDxrnxog7LwP6atIMKZiBKECgSC+IHCzIYEJO/OLrC+nMRQfzBBdDZj9BQ+1ofDG9WFCQ6L9aQd6b4PGK51071wmPVoi21kDRZLor3H+czuYOd9DfRNQuQ+F6aykg29BaTwPoSD+Y8vIbgck3Kh1YE4bvPXSTn730U9xsHuagWKJbYdugRS4hYCFpz3cpTiKB8tnuvjZ4dPUr+RIjetIKajvtZGCSN+n4z7Urmy1DIK0j1FWIBZyoDiLOmOSObzM9FKe7OElrCXBb7z+UV781m76EhVCKeh6QUc4CkiBjAc0+kKCfptPvPgEQSLELkia80nEmkFmawkvIQga9+M27TYTXXughKhpzDczKK5gtZzEirssjxepPGQjVg2SNzTOfGkHc5UM+3/zLORcVNOnr2+d3hMhrMYIMj4i52ItC5586Dx6TWCf7ECEEL91Hy4RYSqk8xsa7vksXaMrnJsYxOnxKOZqOBPpSD11/2vIDpdWp6S5yadxLcuL18aQtsoTW8eZu1Vg5edbFM4IREslm2mgvG+dE9NbiC+Al5I0eyTmkdUNHVtbBFh6CisPSoQvWFlPo1g++rrG2tlOpCYJYyF/9NpjKHpIkAzI9lQJuxys03HQJF+f2kr6qoazEGf5qI+xrqIIcE7lseeSVLdKAjNEHalTP99+Rc97DwGqLYiV4ImRq8iSQahJfvNDXyTUIT2hYayoyGWT7WOzZCwb6Ss097XoeE0nnEiS/8AciYGIVOL0uzhfK2A9tIqMhfTvXkTvauHaGqmbGzu0t0M82XZbzuvbj6oQ4jfupqwXInIzrD7g8OqtrYhAoA02+G+f/BBPP3wWEYBRFYRJn6mXhpi+0k36nEH8vMXawx7qthpBqOCfzZLuqhO/btDqklRrcbSyyuxqlsTXElH/XLnNdNOklOO35bz2AQ8Q8c2e425SpwKBaGgoekhProra1UI5HxFNXpzYTmWXx+MfiezsFA/S1xWaPZGhqqhpuFNJQikIYpLh3DqtbQ6qDVu6V5AqhKUYoS7IvWqy8KH23qY9AUxKKae4i9QpxYf+sSWCqs7iN/voK5SxR2y0h0pokybF/jJ/9a29DAys8bO/8DLFn5oBBTIDFZSCQ883JPW/7sYr+pyf6kNZ1QlHGyx9bhOhIZFGSKtbsnbQZ1t/m/EivgsfBf7s9vN3RJ36TsQyDotnusmfUXE228yu5OjvLlFdTaBXBe7zRbS0y8K5bj75+eMsPj8A/S12d84jZk2qv1SlfqhF/pSGWDMI8j6W6VE5ZEeGJVkbxRMIR2H87OAdDvmd4U5sfw3gJ4G/+Ptu/T7XvqfS+J3ctPqKh9oShJpArBsYMY+ZG0W0uE9zb4v0BxcQQhIkQpyOAP+hGn5N562XdmKuCVpXsoRrBoEh2LZvGnVNpy9TwYy7CF9gr5vYfR4oEGbaN5v2DHBaSvntz9g7ok79P56e8STWA2tUHnLIXBUEgYKS9PAbOvoNi/ILPRgxnycevIg0QrQ3U6S66ui7K2SOL5KYiXQmqtsCri8WeeLxs8z89RBjnUuIAEQsJFWso5cUdKt988HP8n+XB7iL1KlY3MUPFURJp/SAj1MxkaEgWWgQmBL7YIPGUoKKZ7Jz2yzN3pBWM4Z3PsvcrQLV0ZCOcwIE9HZUOPGF/dhFScmJEyRDzKRDfSqDCAReuQ0VT4QQcSJ61Oe/4/Jdo045jk5Xqk4YC8lc0BFGgKJJWs0YfibAX7ZAwsmLW7g02UeY8VGvxek+Mg9aSHxWYeifTmCsKfihgjPWQq8Ibs0UUesKzkyS2LpCuK2OXt7YoufbpU41+S5v5Lsp6yV8weRskeMHLjO+uZPwpW5ix1axX4msyRqDAVZnk3TcZvl6B5lNFezpPLMXutF7WzgFnQsLvTi9HsulFGFLo+fYLDdudCEHbMKWiouGXLWItb7fV8S9Q1uc5KQmkS2NV66NsFJJkn5ykVIlgfNgHalCdqgMZ9KoSohWtCkvpXCGHcKYpLejghhuYK9aqHEf03JRayo3pjrJXNDp7qgQz7UAMDqbKHvvw05PTQ9QGwrx8xZcT7B0tgshJNnnEzS2udgnO/ATkrodw6saqDUVZU1HuAI/VNC0AHNRY/N/jzYrSm8LPIVGn6Rmx3Acnc6xFYJbyQ0fW1sEOJSCxIxCfauHlwkJUiHxt+LYHQKlotHq89l0aJZQCpLXdYKcT2pSQetrsvpWF8cGr+MUAgq/M029HMe0XLaNziE1SXMiizJt0fibLqxFQWtxY4PcHtSpskp1zEdYAbjR/9x7pMpgvsTqnw3SKmhcj3WCryA2BRBC8/E6xqkk9W0ur8xsxZpXeSMzjHk9Rm0LZItLiF4bKQXKjYio0hjyObpngqmNHNsGvtYPhJ61+cmd5xnZMYc1r5IwXSbfGiT8iXW8tGSgd51Nnwcl56LGffo7yjR7Q4Qe4tg6UgHD9HAKIemzMS5/bowwEJgXLP7lh79M4fg8ak3Fl/ehXkSYDQh8lS+c2k+qq06rJ0C4On4mwPGit7hSTeI8G3nLMW9yc60HaYUQCmLn4zR32Mh6jNHds0z6/chuB2XOxO6Q/M5rP05sXifMSN68tGVDx9YWM1jaKmFL46n9FwlDBZFz6ctUUNMe9lQKuamFfzOJokpkIPjgsVNoNQW0EPNmjMZWF3U+xm8f+TLXZroIiy5CkWw5OI0+VEeL+zjdPmgSM7ex2pVto/xX/JV/g+IKjK1VtK9nECFUtvuIUBBbVrF7fIQXHYn1moJej8pAbj7EmlNpjriosQAxa8JAi2TCpjIb6fNoFYXeBxeYmuwkd17l3O/9q/tL+c9zNNKTCqnt67jX01R3u1RHA9AkUpM4hQCR8CHrQdEhGGmiuBDftw4SPvSRb7BtaAHzvEV8QWBabmSfJiRaTSGISxbLKVI9NSpH70P1VaTAyULjXB6tJcBTEEFk95u9qNEzuoJsaMiGRvJbFrlMg+ouj9J8BmkFfPHTj3BtvhMvLXGP1qiXLRAgQoFRFsishzefIHwjh1hsw1zEvYZiBCgHKjzy5AWsRYm5pKE2Bel0C9+EhWtFUj01El0NGg81WZuITu16xgFfYeyDE4QNjWeePomzEMdMunx09ynGds4gDpVR1nTUrhZeQiI3NhXRHgEOQwXnWprTf7qHB/7xebYdm+TR4xeolOPUt/hoNQX3fJZsvIVh+KgtQe/AGp6tkeutMPHcKKmuOq/8yUN0bC7hTSf49LmDXL08gKH5Ef+4HCOIS1T7PsxFAFhLgvw/nOX15/ZSdiwmqwVUPeRjR17HWhKEBqy90Y3dNMjuXWVpPY1S0inNZeCxEvWpDM1eyepchthQjUyugbQifUu1JdDLKnpNwetu75rcvYEn0BuSmZUcXkqycLKH5RO9AHz2i49QP9jCywbY/S6dhSqrN/OYpsev/fjzxFZUTN3ng0e/RXIarBmd5BfS1MdzADTtGO9/6iRjD99k5NgNMmc2dg1ui4OGEfcoHXWIXU3gbm0R+grpczGcULD18xVmn8pS3+yjVjRWqgVEp4PxUpo/OP1+wl11Gt8o8qWBHNmnS7RWkvhJncw18EYb6H+V5QtH9iHqtzWKH69E8iIbhLaYwXHVpb+rRMeRRfJfMzHiHq1OST7bYOKX0sjDFWLLKqmtZUR3ZNBX3hXi72zQnasRmKDnbcrzaYY2rcBQAyRUl5Os7wvBjXYl1oJKq3YfErDLjTjHuidYXE+j/vQKQaAgtjQIQkFsoI5/IUP34QVqdYuufMSbGN0+i/VGgsUz3eh7yoRzcZSkx/ybvSAFleMttLJG4pbKttE5fuzoOdycRDr3YS6CUPDn4wdIf93CNuIwGEZKf7uaVMfzsLVF63/3IA9I5psdEAhuXRvE2e1izhikLRtj1Kd6pQMkeA2dnVvnuNTop6mr3HxtkPldaaQAJbGxVeW2OCoLIWpEKlZ3igLww9AlN0kpiz/E390x2mMGw/gPkxsQQpzaqJzCD4u2WIN/lPFegO8x2iXAf7DBf7dhaIsvuR9ltMsM/pHFewG+x3jXAyyEeFoIMX675eAHKQrekUN520BK+a49ABWYJNLHNIBzwI4fcP8toPBd1/4L8Fu3n/8W8J/fzTF99+PdnsEPAdellDeklC7waaIWhDvB39XK0BZ4twN8p+0Gd+JQ3hZ4t4/Kb6vd4DtwVEo5L4ToBF4SQly9R+/rruHdnsF3pNQqv8OhnKiV7G8dyuF7WhnaAu92gE8CI0KI4dtNNh8lakH4HvwQDuVtgXd1iZBS+kKIXwVeINpR/LGU8tLfcXsX8FykwY8GfEpK+bwQ4iTwGSHEPwGmgZ/ZgLf+tvHeUfke491eIn7k8V6A7zHeC/A9xnsBvsd4L8D3GO8F+B7jvQDfY/wfhojopBOFWdEAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for l in m.layers:\n",
    "    plt.imshow(l.weights)\n",
    "    plt.title(l.name)\n",
    "    plt.show()\n",
    "    #plt.imshow(l.biases)\n",
    "    #plt.title(l.name)\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T02:50:45.751533Z",
     "start_time": "2020-05-25T02:50:45.686576Z"
    },
    "code_folding": [
     1,
     11,
     20,
     25,
     41,
     53,
     65
    ]
   },
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, layers, name=None, learning_rate = 0.01, mr=0.001):\n",
    "        self.name = name\n",
    "        self.learning_rate = learning_rate\n",
    "        self.mr = mr\n",
    "        keys = [\"sgd\", \"iterative\", \"momentum\", \"rmsprop\", \"adagrad\", \"adam\", \"adamax\", \"adadelta\"]\n",
    "        values = [self.sgd, self.iterative, self.momentum, self.rmsprop, self.adagrad, self.adam, self.adamax, self.adadelta]\n",
    "        self.opt_dict = {keys[i]:values[i] for i in range(len(keys))}\n",
    "        if name != None and name in keys:\n",
    "            self.opt_dict[name](layers=layers, training=False)\n",
    "            #pass\n",
    "    def sgd(self, layers, learning_rate=0.01, beta=0.001, training=True):\n",
    "        learning_rate = self.learning_rate\n",
    "        for l in layers:\n",
    "            if l.parameters !=0:\n",
    "                if training:\n",
    "                    l.weights += l.pdelta_weights*self.mr + l.delta_weights * learning_rate\n",
    "                    l.biases += l.pdelta_biases*self.mr + l.delta_biases * learning_rate\n",
    "                    l.pdelta_weights = l.delta_weights\n",
    "                    l.pdelta_biases = l.delta_biases\n",
    "    def iterative(self, layers, learning_rate=0.01, beta=0, training=True):\n",
    "        for l in layers:\n",
    "            if l.parameters !=0:\n",
    "                l.weights -= learning_rate * l.delta_weights\n",
    "                l.biases -= learning_rate * l.delta_biases\n",
    "    def momentum(self, layers, learning_rate=0.1, beta1=0.9, weight_decay=0.0005, nesterov=True, training=True):\n",
    "        learning_rate = self.learning_rate\n",
    "        #beta1 = 1 - self.learning_rate\n",
    "        for l in layers:\n",
    "            if l.parameters !=0:\n",
    "                if training:\n",
    "                    l.weights_momentum = beta1 * l.weights_momentum + learning_rate * l.delta_weights-weight_decay *learning_rate*l.weights\n",
    "                    l.weights+=l.weights_momentum\n",
    "                    #\n",
    "                    l.biases_momentum = beta1 * l.biases_momentum + learning_rate * l.delta_biases-weight_decay *learning_rate*l.biases\n",
    "                    l.biases+=l.biases_momentum\n",
    "                else:\n",
    "                    l.weights_momentum = 0\n",
    "                    l.biases_momentum = 0\n",
    "\n",
    "            \n",
    "    def rmsprop(self, layers, learning_rate=0.001, beta1=0.9, epsilon=1e-8, training=True):\n",
    "        learning_rate=self.learning_rate\n",
    "        for l in layers:\n",
    "            if l.parameters !=0:\n",
    "                if training:\n",
    "                    l.weights_rms = beta1*l.weights_rms + (1-beta1)*(l.delta_weights ** 2)\n",
    "                    l.weights += learning_rate * (l.delta_weights/np.sqrt(l.weights_rms + epsilon))\n",
    "                    l.biases_rms = beta1*l.biases_rms + (1-beta1)*(l.delta_biases ** 2)\n",
    "                    l.biases += learning_rate * (l.delta_biases/np.sqrt(l.biases_rms + epsilon))\n",
    "                else:\n",
    "                    l.weights_rms = 0\n",
    "                    l.biases_rms = 0\n",
    "    def adagrad(self, layers, learning_rate=0.01, beta1=0.9, epsilon=1e-8, training=True):\n",
    "        learning_rate=self.learning_rate\n",
    "        for l in layers:\n",
    "            if l.parameters != 0:\n",
    "                if training:\n",
    "                    l.weights_adagrad += l.delta_weights ** 2\n",
    "                    l.weights += learning_rate * (l.delta_weights/np.sqrt(l.weights_adagrad+epsilon))\n",
    "                    l.biases_adagrad += l.delta_biases ** 2\n",
    "                    l.biases += learning_rate * (l.delta_biases/np.sqrt(l.biases_adagrad+epsilon))\n",
    "                else:\n",
    "                    l.weights_adagrad = 0\n",
    "                    l.biases_adagrad = 0\n",
    "    def adam(self, layers, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, decay=0, training=True):\n",
    "        #print(training)\n",
    "        for l in layers:\n",
    "            if l.parameters != 0:\n",
    "                if training:\n",
    "                    l.t += 1\n",
    "                    if l.t == 1:\n",
    "                        l.pdelta_biases = 0\n",
    "                        l.pdelta_weights = 0\n",
    "                    l.weights_adam1 = beta1 * l.weights_adam1 + (1-beta1)*l.delta_weights\n",
    "                    l.weights_adam2 = beta2 * l.weights_adam2 + (1-beta2)*(l.delta_weights**2)\n",
    "                    mcap = l.weights_adam1/(1-beta1**l.t)\n",
    "                    vcap = l.weights_adam2/(1-beta2**l.t)\n",
    "                    l.delta_weights = mcap/(np.sqrt(vcap) + epsilon)\n",
    "                    l.weights += l.pdelta_weights * self.mr + learning_rate * l.delta_weights\n",
    "                    l.pdelta_weights = l.delta_weights * 0\n",
    "\n",
    "                    l.biases_adam1 = beta1 * l.biases_adam1 + (1-beta1)*l.delta_biases\n",
    "                    l.biases_adam2 = beta2 * l.biases_adam2 + (1-beta2)*(l.delta_biases**2)\n",
    "                    mcap = l.biases_adam1/(1-beta1**l.t)\n",
    "                    vcap = l.biases_adam2/(1-beta2**l.t)\n",
    "                    l.delta_biases = mcap/(np.sqrt(vcap) +epsilon)\n",
    "                    l.biases += l.pdelta_biases * self.mr + learning_rate * l.delta_biases\n",
    "                    l.pdelta_biases = l.delta_biases * 0\n",
    "                    \n",
    "                else:\n",
    "                    l.t = 0\n",
    "                    l.weights_adam1 = 0\n",
    "                    l.weights_adam2 = 0\n",
    "                    l.biases_adam1 = 0\n",
    "                    l.biases_adam2 = 0\n",
    "                    \n",
    "    def adamax(self, layers, learning_rate=0.002, beta1=0.9, beta2=0.999, epsilon=1e-8, training=True):\n",
    "        pass\n",
    "    def adadelta(self, layers, learning_rate=0.01, beta1=0.9, epsilon=1e-8, training=True):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T06:21:22.152240Z",
     "start_time": "2020-05-22T06:21:20.047Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('ffndata.csv', header=None)\n",
    "X = np.array(data[[0,1]])\n",
    "y = np.array(data[2]).reshape(-1, 1) / 1.0\n",
    "#print(X.shape, y.shape)\n",
    "model = NN()\n",
    "model.add(FFL(n_input=2, neurons=1, activation=\"sigmoid\"))\n",
    "# model.add(FFL(1, activation=\"sigmoid\"))\n",
    "model.compile_model(lr=0.01, opt=\"sgd\", loss=\"cse\", mr= 0.001)\n",
    "model.summary()\n",
    "model.train(X, y, epochs = 1000, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T15:01:29.808686Z",
     "start_time": "2020-05-24T15:01:16.712376Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Input  Neurons Activation  Bias\n",
      "Layer Name                                  \n",
      "Input Layer      1        1       None  True\n",
      "Total Parameters:  2\n",
      "None\n",
      "Time: 0.017sec\n",
      "Epoch: #0, Loss: 2.732500\n",
      "Accuracy: 0.0%\n",
      "None\n",
      "Time: 5.355sec\n",
      "Epoch: #1000, Loss: 0.000000\n",
      "Accuracy: 1.0%\n",
      "None\n",
      "Time: 5.241sec\n",
      "Epoch: #2000, Loss: 0.000000\n",
      "Accuracy: 100.0%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-ae46d29bb9c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mmult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"sgd\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"mse\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmr\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mmult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mshow_every\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-24-7e96f791ceaf>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X, Y, epochs, show_every, batch_size, shuffle)\u001b[0m\n\u001b[0;32m    281\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m                 \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m                 \u001b[0mcurr_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurr_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m                 \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m                 \u001b[0mbatch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x = np.arange(0, 100).reshape(-1, 1)\n",
    "x = x / x.max()\n",
    "#y = np.repeat(np.array([[1, 0]]), 100, axis=0).reshape(100, 2)\n",
    "#y = np.vstack([y, np.repeat(np.array([[0, 1]]), 100, axis=0).reshape(100, 2)])\n",
    "y = x * 2 \n",
    "#print(x.shape, y.shape)\n",
    "mult = NN()\n",
    "mult.add(FFL(1, 1))\n",
    "#mult.add(FFL(neurons=10, activation=\"sigmoid\"))\n",
    "# mult.add(FFL(neurons=1))\n",
    "mult.summary()\n",
    "\n",
    "mult.compile_model(lr=0.01, opt=\"sgd\", loss=\"mse\", mr= 0.001)\n",
    "mult.train(x, y, 10000,  show_every=1000, batch_size = 8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T12:43:34.714962Z",
     "start_time": "2020-05-24T12:43:34.705981Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[2000.]])], [array([[2.]])])"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mult.predict([1000]), [l.weights for l in mult.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T06:21:22.159236Z",
     "start_time": "2020-05-22T06:21:20.200Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def golorot(shape):\n",
    "    wts = np.random.randint(-6, 6, np.multiply.reduce(shape))/np.add.reduce(shape)\n",
    "    return wts.reshape(shape)\n",
    "golorot((5, 5, 3))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T06:21:22.161235Z",
     "start_time": "2020-05-22T06:21:20.266Z"
    }
   },
   "outputs": [],
   "source": [
    "x.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T14:15:16.203728Z",
     "start_time": "2020-05-20T14:15:16.199730Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T06:21:22.164234Z",
     "start_time": "2020-05-22T06:21:20.331Z"
    }
   },
   "outputs": [],
   "source": [
    "y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T06:21:22.166233Z",
     "start_time": "2020-05-22T06:21:20.369Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
